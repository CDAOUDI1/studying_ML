{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca314891",
   "metadata": {},
   "source": [
    "### Manipulation de données avancées \n",
    "\n",
    "Cette  partie du cours introduira les objectifs clés et les concepts fondamentaux pour manipuler des données volumineuses en utilisant **Pandas** et **NumPy**, tout en optimisant les performances. L'accent sera mis sur la compréhension des techniques avancées et leur application pratique.\n",
    "\n",
    "### **Objectifs du Cours :**\n",
    "\n",
    "1. **Maîtriser les concepts avancés de Pandas et NumPy pour manipuler et traiter de grandes quantités de données :**\n",
    "   - **Pandas** et **NumPy** sont les bibliothèques principales utilisées pour le traitement de données en Python. **Pandas** est parfait pour manipuler des données structurées (comme les tables de données), tandis que **NumPy** est utilisé pour le calcul scientifique sur de grands tableaux de données numériques.\n",
    "   - **Exemple :**\n",
    "     - **Pandas :** Imaginons un fichier CSV contenant des informations de ventes d'une entreprise sur plusieurs années. À l'aide de **Pandas**, nous pouvons facilement charger ce fichier en un DataFrame, trier les données, appliquer des transformations, et calculer des statistiques sur des périodes spécifiques.\n",
    "     - **NumPy :** Si nous avons un tableau de **NumPy** représentant des mesures météorologiques (température, humidité, etc.), nous pouvons appliquer des calculs de moyennes ou d'écarts-types sur des colonnes entières en une seule ligne de code grâce à la vectorisation.\n",
    "   - **Exercice :** Charger un fichier CSV avec Pandas et appliquer une fonction de base de données (par exemple, la somme des ventes par mois ou par produit).\n",
    "2. **Optimiser les performances des opérations sur de grands jeux de données :**\n",
    "   - Lorsque l'on travaille avec de grands volumes de données (plusieurs millions de lignes, voire des gigaoctets de données), il est important d'optimiser les performances pour éviter des temps de traitement trop longs. Cette optimisation passe par l'utilisation de structures de données efficaces et de techniques de calculs parallèles.\n",
    "   - **Exemple :**\n",
    "     - Lorsque vous travaillez avec un DataFrame Pandas volumineux, vous pouvez optimiser la mémoire en utilisant des types de données adaptés (comme **`float32`** ou **`category`** pour les colonnes catégorielles) au lieu des types standards.\n",
    "     - Si vous devez charger des fichiers volumineux CSV, au lieu de tout charger en mémoire d'un coup, vous pouvez utiliser **`chunksize`** pour charger les données en petits morceaux (par exemple, 100 000 lignes à la fois), puis traiter chaque morceau indépendamment.\n",
    "   - **Exercice :** Chargement et traitement d'un fichier CSV volumineux en utilisant l'argument **`chunksize`** et optimisation des types de colonnes pour réduire l'utilisation de la mémoire.\n",
    "3. **Utiliser des techniques avancées d'indexation, de fusion, et de transformation :**\n",
    "   - **Indexation avancée** : Pandas permet de sélectionner des données de manière très flexible grâce à l'indexation par étiquette (`.loc[]`) et par position (`.iloc[]`), et avec la possibilité de manipuler plusieurs niveaux d'index (Multindex).\n",
    "   - **Fusion de DataFrames** : L'outil `merge()` de Pandas permet de combiner plusieurs DataFrames sur des colonnes clés communes (comme une jointure en SQL).\n",
    "   - **Transformation des données** : Vous pouvez appliquer des transformations complexes sur les données à l'aide de méthodes comme **`apply()`**, **`map()`**, ou **`applymap()`**.\n",
    "   - **Exemple :**\n",
    "     - **Indexation :** Imaginez un DataFrame contenant des informations sur les employés d'une entreprise, avec des indices hiérarchiques sur les départements. Vous pourriez utiliser **`df.loc[(\"Marketing\", \"John Doe\"), \"salary\"]`** pour obtenir le salaire d'un employé spécifique.\n",
    "     - **Fusion :** Si vous avez un DataFrame avec des informations sur les employés et un autre avec des informations sur les départements, vous pouvez les fusionner en utilisant une **jointure interne** sur la colonne **`department_id`** pour obtenir un seul DataFrame avec toutes les informations pertinentes.\n",
    "     - **Transformation :** Pour calculer une nouvelle colonne **`bonus`** en fonction du salaire et des années d'expérience, vous pouvez utiliser **`df[\"bonus\"] = df.apply(lambda x: x[\"salary\"] \\* 0.1 if x[\"years_experience\"] > 5 else 0, axis=1)`**.\n",
    "   - **Exercice :** Effectuer une jointure entre deux DataFrames et transformer les données à l'aide de **`apply()`**.\n",
    "4. **Apprendre à appliquer des méthodes de nettoyage, d'agrégation et de rééchantillonnage :**\n",
    "   - Le nettoyage des données est une étape cruciale dans le traitement des données, surtout lorsque les jeux de données contiennent des valeurs manquantes, des doublons ou des valeurs erronées.\n",
    "   - **Agrégation** : L'agrégation des données (par exemple, en utilisant la méthode **`groupby()`**) permet de résumer les données par des catégories spécifiques (comme des sommes, des moyennes, ou des comptages).\n",
    "   - **Rééchantillonnage** : Il s'agit de la pratique consistant à regrouper les données à un niveau plus élevé ou plus bas (par exemple, rééchantillonner des séries temporelles par mois ou par année).\n",
    "   - **Exemple :**\n",
    "     - **Nettoyage des données :** Si vous avez un DataFrame avec des valeurs manquantes dans plusieurs colonnes, vous pouvez utiliser **`df.fillna()`** pour remplir ces valeurs, ou **`df.dropna()`** pour les supprimer.\n",
    "     - **Agrégation :** Supposons que vous ayez des données de vente par produit et par date. Vous pouvez grouper par produit et calculer la **moyenne des ventes** avec **`df.groupby(\"product_id\").mean()`**.\n",
    "     - **Rééchantillonnage :** Si vous avez une série temporelle avec des données horaires et que vous souhaitez obtenir la **moyenne quotidienne**, vous pouvez utiliser **`df.resample('D').mean()`**.\n",
    "   - **Exercice :** Appliquer une fonction d'agrégation (par exemple, moyenne ou somme) sur des groupes de données et rééchantillonner une série temporelle.\n",
    "5. **Découvrir des techniques de traitement parallèle pour les gros volumes de données :**\n",
    "   - Lorsque vous travaillez avec de très grandes quantités de données, le traitement parallèle permet de diviser le travail en plusieurs tâches exécutées simultanément, ce qui accélère considérablement le processus.\n",
    "   - Vous pouvez utiliser des bibliothèques comme **Dask** ou **Modin** pour paralléliser vos calculs sans changer votre code Pandas habituel.\n",
    "   - **Exemple :**\n",
    "     - Si vous avez un fichier CSV massif, vous pouvez utiliser **Dask DataFrame** pour effectuer des opérations de manière parallèle, ce qui réduit les temps de calcul.\n",
    "     - **Modin** permet de faire des calculs sur de grands DataFrames Pandas en parallèle sur plusieurs cœurs sans modifications majeures du code.\n",
    "   - **Exercice :** Utiliser **Dask** pour charger un fichier volumineux en parallèle et effectuer des opérations sur les données.\n",
    "\n",
    "\n",
    "\n",
    "### Partie 1 : Présentation des Structures de Données Pandas et NumPy \n",
    "\n",
    "Dans cette première section du cours, nous explorerons les structures de données fondamentales de **Pandas** et **NumPy**. Ces deux bibliothèques sont essentielles pour le traitement de données en Python et offrent des structures puissantes pour organiser, manipuler et analyser les données.\n",
    "\n",
    "### 1.1 Structures de Données de Pandas et NumPy \n",
    "\n",
    "#### 1.1.1 DataFrame et Series : Concepts et exemples de base\n",
    "\n",
    "- **Series (Pandas) :**\n",
    "\n",
    "  - Une **Series** est une structure de données unidimensionnelle qui peut contenir n'importe quel type de données (entiers, flottants, chaînes, objets Python, etc.). Elle est similaire à une liste ou à un tableau 1D, mais elle est associée à un **index**, ce qui permet de référencer ses éléments de manière plus flexible.\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "521ee99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10\n",
      "1    20\n",
      "2    30\n",
      "3    40\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "    import pandas as pd\n",
    "    data = [10, 20, 30, 40]\n",
    "    series = pd.Series(data)\n",
    "    print(series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b6d27",
   "metadata": {},
   "source": [
    "\n",
    "- **DataFrame (Pandas) :**\n",
    "\n",
    "  - Un **DataFrame** est une structure de données bidimensionnelle qui ressemble à une table (tableau avec lignes et colonnes). Chaque colonne d'un DataFrame est en réalité une **Series**.\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01faa86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Âge    Ville\n",
      "0    Alice   25    Paris\n",
      "1      Bob   30  Londres\n",
      "2  Charlie   35   Berlin\n"
     ]
    }
   ],
   "source": [
    "    data = {'Nom': ['Alice', 'Bob', 'Charlie'],\n",
    "            'Âge': [25, 30, 35],\n",
    "            'Ville': ['Paris', 'Londres', 'Berlin']}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06dd9b6",
   "metadata": {},
   "source": [
    "\n",
    "#### **1.1.2 Différences entre Pandas et NumPy : Quand utiliser l'un ou l'autre**\n",
    "\n",
    "- **NumPy** est une bibliothèque principalement utilisée pour les calculs numériques avec des **tableaux multidimensionnels**. Il est optimisé pour les opérations mathématiques sur de grandes quantités de données, comme les matrices ou les vecteurs.\n",
    "\n",
    "  - Utilisez **NumPy** quand vous avez besoin de faire des calculs mathématiques rapides sur des tableaux de données numériques, ou si vous travaillez avec des données homogènes.\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b8e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "    import numpy as np\n",
    "    array = np.array([1, 2, 3, 4])\n",
    "    print(array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f83cc",
   "metadata": {},
   "source": [
    "\n",
    "- **Pandas**, quant à lui, est plus adapté pour manipuler des **données hétérogènes** (nombres, chaînes de caractères, etc.) organisées sous forme de tableaux (DataFrame et Series). Il permet une manipulation flexible des données avec un grand nombre de méthodes pour nettoyer, filtrer, transformer, et agrégérer des données.\n",
    "\n",
    "  - Utilisez **Pandas** lorsque vous devez travailler avec des données structurées (par exemple, un tableau Excel ou un fichier CSV) et effectuer des opérations complexes de nettoyage, transformation, ou agrégation.\n",
    "\n",
    "#### **1.1.3 Création de DataFrames et Series à partir de listes, dictionnaires et fichiers CSV**\n",
    "\n",
    "- **A partir de listes et dictionnaires :**\n",
    "\n",
    "  - Nous pouvons créer des **DataFrames** à partir de listes ou dictionnaires, ce qui est très utile pour des jeux de données simples ou pour manipuler de petits échantillons de données.\n",
    "\n",
    "  - **Exemple avec un dictionnaire :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d2015b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Age\n",
      "0    Alice   25\n",
      "1      Bob   30\n",
      "2  Charlie   35\n"
     ]
    }
   ],
   "source": [
    "    data = {'Nom': ['Alice', 'Bob', 'Charlie'],\n",
    "            'Age': [25, 30, 35]}\n",
    "    df = pd.DataFrame(data)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea3ff6b",
   "metadata": {},
   "source": [
    "\n",
    "- **A partir de fichiers CSV :**\n",
    "\n",
    "  - Pandas offre une méthode **`pd.read_csv()`** qui permet de charger des données directement depuis un fichier CSV dans un **DataFrame**.\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3d54b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    nom prenom  age\n",
      "0  said    xxx   22\n",
      "1  moha      y   23\n"
     ]
    }
   ],
   "source": [
    "    df = pd.read_csv('data/file.csv')\n",
    "    print(df.head())  # Afficher les premières lignes du DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b293b",
   "metadata": {},
   "source": [
    "\n",
    "#### Exercice pratique \n",
    "\n",
    "**Objectif :** Créer un DataFrame simple à partir d'un dictionnaire et explorer ses différentes propriétés.\n",
    "\n",
    "- Demandez aux étudiants de créer un DataFrame à partir d'un dictionnaire contenant les informations suivantes : `Nom`, `Âge`, `Ville`.\n",
    "- Après cela, demandez-leur d'explorer les propriétés du DataFrame comme :\n",
    "  - Afficher les premières lignes avec **`df.head()`**.\n",
    "  - Afficher les types des colonnes avec **`df.dtypes`**.\n",
    "  - Accéder à une colonne spécifique en utilisant la syntaxe de clé : **`df['Nom']`**.\n",
    "\n",
    "### 1.2 Indexation Avancée avec Pandas \n",
    "\n",
    "L'indexation est l'un des aspects les plus puissants de **Pandas**. Comprendre comment utiliser correctement **`loc[]`**, **`iloc[]`**, et **Multindex** permet de manipuler des données complexes de manière efficace.\n",
    "\n",
    "#### 1.2.1 Indexation par étiquette avec `.loc[]` et indexation par position avec `.iloc[]`\n",
    "\n",
    "- **`.loc[]`** : Cette méthode permet de sélectionner des lignes et des colonnes par leurs **étiquettes** (noms). Elle est très utile lorsque vous travaillez avec des index nommés.\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "138be084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "a  1  4\n",
       "b  2  5\n",
       "c  3  6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=['a', 'b', 'c'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68862595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "a  1  4\n",
       "c  3  6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[['a','c']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f9109f",
   "metadata": {},
   "source": [
    "\n",
    "- **`.iloc[]`** : Cette méthode permet de sélectionner des lignes et des colonnes par leur **position** (index entier).\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a38dcc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "a  1  4\n",
      "b  2  5\n"
     ]
    }
   ],
   "source": [
    "    print(df.iloc[[0,1]])  # Sélectionne la première ligne (index 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67290bb",
   "metadata": {},
   "source": [
    "\n",
    "#### **1.2.2 Indexation conditionnelle avec des expressions booléennes**\n",
    "\n",
    "- Vous pouvez également filtrer les données avec des  expressions booléennes. Cela permet de sélectionner uniquement les lignes qui satisfont une condition spécifique.\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eec7cc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "0  1  4\n",
       "1  2  5\n",
       "2  3  6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df6fa8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "1  2  5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = df[(df['A'] > 1) & (df['B'] ==5)]  # Sélectionne les lignes où la valeur dans la colonne 'A' est supérieure à 1\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f93f1e",
   "metadata": {},
   "source": [
    "\n",
    "#### **1.2.3 Multindex : Utilisation de plusieurs niveaux d'indexation pour des structures complexes**\n",
    "\n",
    "- Multindex\n",
    "\n",
    "   permet d'avoir plusieurs niveaux d'index dans un DataFrame. Cela est utile lorsqu'on a des données hiérarchiques.\n",
    "\n",
    "  - Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "857d465d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           value\n",
      "lettr num       \n",
      "A     1       10\n",
      "      2       20\n",
      "      4       30\n",
      "B     1       50\n"
     ]
    }
   ],
   "source": [
    "    index = pd.MultiIndex.from_tuples([('A', 1), ('A', 2), ('A', 4), ('B', 1)], \n",
    "    names=['lettr', 'num'])\n",
    "    df = pd.DataFrame({'value': [10, 20, 30,50]}, index=index)\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "065e26a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lettr</th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value\n",
       "lettr num       \n",
       "A     2       20\n",
       "B     1       50"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[[('A',2), ('B',1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c741e67f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value    50\n",
       "Name: (B, 1), dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "696c6899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Âge    Ville\n",
      "0    Alice   25    Paris\n",
      "1      Bob   30  Londres\n",
      "2  Charlie   35   Berlin\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création d'un dictionnaire\n",
    "data = {'Nom': ['Alice', 'Bob', 'Charlie'],\n",
    "        'Âge': [25, 30, 35],\n",
    "        'Ville': ['Paris', 'Londres', 'Berlin']}\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffcab82",
   "metadata": {},
   "source": [
    "1. **Explorer les propriétés du DataFrame :**\n",
    "\n",
    "- **Afficher les premières lignes du DataFrame :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cad6b45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Âge    Ville\n",
      "0    Alice   25    Paris\n",
      "1      Bob   30  Londres\n",
      "2  Charlie   35   Berlin\n"
     ]
    }
   ],
   "source": [
    "print(df.head())  # Par défaut, head() affiche les 5 premières lignes, ici on en a 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7b569f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Âge    Ville\n",
      "0    Alice   25    Paris\n",
      "1      Bob   30  Londres\n",
      "2  Charlie   35   Berlin\n"
     ]
    }
   ],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "906cc1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom      object\n",
      "Âge       int64\n",
      "Ville    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a1cd077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Nom     3 non-null      object\n",
      " 1   Âge     3 non-null      int64 \n",
      " 2   Ville   3 non-null      object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 200.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc5a9a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Âge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>32.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Âge\n",
       "count   3.0\n",
       "mean   30.0\n",
       "std     5.0\n",
       "min    25.0\n",
       "25%    27.5\n",
       "50%    30.0\n",
       "75%    32.5\n",
       "max    35.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33875f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Alice\n",
      "1        Bob\n",
      "2    Charlie\n",
      "Name: Nom, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner la colonne 'Nom'\n",
    "print(df['Nom'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a579ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Nom', 'Âge', 'Ville'], dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "415d2db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Ville</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>Londres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom    Ville\n",
       "0    Alice    Paris\n",
       "1      Bob  Londres\n",
       "2  Charlie   Berlin"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[\"Nom\", \"Ville\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13f5e480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B\n",
       "a  1  4\n",
       "b  2  5\n",
       "c  3  6"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sélectionner la ligne avec l'étiquette 'a' (en supposant que df a un index personnalisé)\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, index=['a', 'b', 'c'])\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9124bb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    4\n",
      "Name: a, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sélection de la ligne où l'index est 'a'\n",
    "print(df.loc['a'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab2fc6",
   "metadata": {},
   "source": [
    "\n",
    "- **Sélection de colonnes spécifiques avec `.loc[]` :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "07ef128e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    4\n",
      "Name: a, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner les colonnes 'A' et 'B' pour la ligne avec l'index 'a'\n",
    "print(df.loc['a', ['A','B']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c99d778",
   "metadata": {},
   "source": [
    "\n",
    "1. **Sélection avec `.iloc[]` (par position) :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "daddc179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    4\n",
      "Name: a, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner la première ligne (index positionnel 0)\n",
    "print(df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7389edb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Sélection de la première ligne et des deux premières colonnes avec `.iloc[]` :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "82e1a6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A    1\n",
      "B    4\n",
      "Name: a, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner la première ligne et les deux premières colonnes\n",
    "print(df.iloc[0, :2])  # '0' pour la première ligne, ':2' pour les deux premières colonnes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c499174e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Exercice  : Utiliser un Multindex pour explorer des jeux de données complexes**\n",
    "\n",
    "**Objectif :** Utiliser un **Multindex** pour organiser et explorer des jeux de données complexes.\n",
    "\n",
    "#### **Solution :**\n",
    "\n",
    "1. **Création d'un DataFrame avec un \\**Multindex\\** :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "81676a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           value\n",
      "lettr num       \n",
      "A     1       10\n",
      "      2       20\n",
      "B     1       30\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création d'un Multindex\n",
    "index = pd.MultiIndex.from_tuples([('A', 1), ('A', 2), ('B', 1)], names=['lettr', 'num'])\n",
    "\n",
    "# Création du DataFrame avec le Multindex\n",
    "df = pd.DataFrame({'value': [10, 20, 30]}, index=index)\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c32996",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Accéder aux données avec le \\**Multindex\\** :**\n",
    "\n",
    "- Sélectionner une ligne en utilisant les deux niveaux de l'index :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ca237f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value    10\n",
      "Name: (A, 1), dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner la valeur associée à l'index ('A', 1)\n",
    "print(df.loc[('A', 1)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058e8ec",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Sélectionner toutes les lignes sous le premier niveau d'index ('A') :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "003e8f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     value\n",
      "num       \n",
      "1       10\n",
      "2       20\n"
     ]
    }
   ],
   "source": [
    "# Sélectionner toutes les lignes où 'lettr' est égal à 'A'\n",
    "print(df.loc['A'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce97eca",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Partie 2 : Nettoyage et Transformation des Données \n",
    "\n",
    "#### 2.1 Nettoyage de Données \n",
    "\n",
    "Le nettoyage de données est une étape essentielle dans le traitement des données avant d'effectuer des analyses ou des transformations. Voici les concepts clés pour cette partie du cours :\n",
    "\n",
    "### **Gestion des valeurs manquantes**\n",
    "\n",
    "Les valeurs manquantes sont courantes dans les jeux de données. Pandas offre plusieurs méthodes pour les gérer :\n",
    "\n",
    "1. **`dropna()`** : Supprime les lignes ou les colonnes contenant des valeurs manquantes.\n",
    "\n",
    "   Exemple :\n",
    "\n",
    "   - Pour supprimer les lignes avec des valeurs manquantes :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b2d990c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lettr</th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">A</th>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value\n",
       "lettr num       \n",
       "A     1       10\n",
       "      2       20\n",
       "B     1       30"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afda8dc",
   "metadata": {},
   "source": [
    "\n",
    "2. **`fillna()`** : Remplir les valeurs manquantes par une valeur spécifiée (par exemple, 0, la moyenne, ou la médiane).\n",
    "\n",
    "   Exemple :\n",
    "\n",
    "   - Remplir les valeurs manquantes avec 0 :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0441df13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lettr</th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">A</th>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value\n",
       "lettr num       \n",
       "A     1       10\n",
       "      2       20\n",
       "B     1       30"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0eb6fd",
   "metadata": {},
   "source": [
    "\n",
    "   - Remplir les valeurs manquantes d'une colonne avec la moyenne de la colonne :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f024ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['colonne'] = df['value'].fillna(df['value'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44da8f7",
   "metadata": {},
   "source": [
    "\n",
    "3. **`isna()`** : Retourne un DataFrame de la même forme, mais avec des valeurs booléennes (`True` pour les valeurs manquantes et `False` pour les autres).\n",
    "\n",
    "   Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f148dce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>colonne</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lettr</th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">A</th>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value  colonne\n",
       "lettr num                \n",
       "A     1    False    False\n",
       "      2    False    False\n",
       "B     1    False    False"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   df.isna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed003826",
   "metadata": {},
   "source": [
    "\n",
    "   Cela permet d'identifier rapidement où se trouvent les valeurs manquantes.\n",
    "\n",
    "### **Transformation de types de données**\n",
    "\n",
    "Souvent, vous devrez changer le type des colonnes pour une analyse ou une transformation plus appropriée. Cela peut être fait en utilisant la méthode `astype()`.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "- Conversion d'une colonne en `float` :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f79c3d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 3 entries, ('A', 1) to ('B', 1)\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   value    3 non-null      float32\n",
      " 1   colonne  3 non-null      int64  \n",
      "dtypes: float32(1), int64(1)\n",
      "memory usage: 504.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "  df['value'] = df['value'].astype(np.float32)\n",
    "  df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90aaa0",
   "metadata": {},
   "source": [
    "\n",
    "- Conversion de plusieurs colonnes :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bcbc960d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df = df.astype({'value': np.float64, \n",
    "                  'colonne': 'int64'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6e463",
   "metadata": {},
   "source": [
    "\n",
    "Cela peut être utile lorsqu'on travaille avec des données issues de fichiers CSV ou d'autres sources où les types de colonnes peuvent être incorrects (par exemple, des nombres stockés sous forme de chaînes de caractères).\n",
    "\n",
    "### **Déduplication avec `drop_duplicates()`**\n",
    "\n",
    "Il est fréquent de rencontrer des doublons dans les jeux de données, surtout après des fusions ou des jointures. Pandas propose `drop_duplicates()` pour supprimer ces doublons.\n",
    "\n",
    "Exemple :\n",
    "\n",
    "- Suppression des doublons dans l'ensemble du DataFrame :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cd8298d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>colonne</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lettr</th>\n",
       "      <th>num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">A</th>\n",
       "      <th>1</th>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <th>1</th>\n",
       "      <td>30.0</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           value  colonne\n",
       "lettr num                \n",
       "A     1     10.0       10\n",
       "      2     20.0       20\n",
       "B     1     30.0       30"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79343e0",
   "metadata": {},
   "source": [
    "\n",
    "- Suppression des doublons sur une ou plusieurs colonnes spécifiques :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3daac604",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Index(['colonne1', 'colonne2'], dtype='object')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27672\\1734304014.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'colonne1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'colonne2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[0;32m   6518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6519\u001b[0m         \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"inplace\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6520\u001b[0m         \u001b[0mignore_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ignore_index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6522\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6523\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6524\u001b[0m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6525\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, subset, keep)\u001b[0m\n\u001b[0;32m   6650\u001b[0m         \u001b[1;31m# Otherwise, raise a KeyError, same as if you try to __getitem__ with a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6651\u001b[0m         \u001b[1;31m# key that doesn't exist.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6652\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6653\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6654\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6655\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6656\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6657\u001b[0m             \u001b[1;31m# GH#45236 This is faster than get_group_index below\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: Index(['colonne1', 'colonne2'], dtype='object')"
     ]
    }
   ],
   "source": [
    "  df.drop_duplicates(subset=['colonne1', 'colonne2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50930b55",
   "metadata": {},
   "source": [
    "\n",
    "- Garder seulement la première ou la dernière occurrence d’un doublon :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947df022",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df.drop_duplicates(keep='first')  # ou keep='last'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949a258b",
   "metadata": {},
   "source": [
    "\n",
    "### Exercice Pratique\n",
    "\n",
    "**Objectif :** Appliquer les méthodes pour gérer les valeurs manquantes, transformer des types de données, et supprimer des doublons dans un DataFrame.\n",
    "\n",
    "#### **Étapes de l'exercice :**\n",
    "\n",
    "1. **Création d'un DataFrame avec des valeurs manquantes et des doublons :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bd990",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "c:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame initial:\n",
      "       Nom   Age    Ville\n",
      "0    Alice  25.0    Paris\n",
      "1      Bob   NaN  Londres\n",
      "2  Charlie  35.0   Berlin\n",
      "3      Bob  30.0  Londres\n",
      "4    Alice   NaN    Paris\n",
      "5    David  40.0   Berlin\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Création d'un DataFrame avec des valeurs manquantes et des doublons\n",
    "data = {\n",
    "    'Nom': ['Alice', 'Bob', 'Charlie', 'Bob', 'Alice', 'David'],\n",
    "    'Age': [25, np.nan, 35, 30, np.nan, 40],\n",
    "    'Ville': ['Paris', 'Londres', 'Berlin', 'Londres', 'Paris', 'Berlin']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(\"DataFrame initial:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477dc0fc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1.**Étapes de nettoyage :**\n",
    "\n",
    "- **Gérer les valeurs manquantes :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0a30be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame après gestion des valeurs manquantes:\n",
      "       Nom   Age    Ville\n",
      "0    Alice  25.0    Paris\n",
      "1      Bob  32.5  Londres\n",
      "2  Charlie  35.0   Berlin\n",
      "3      Bob  30.0  Londres\n",
      "4    Alice  32.5    Paris\n",
      "5    David  40.0   Berlin\n"
     ]
    }
   ],
   "source": [
    "# Remplir les valeurs manquantes dans la colonne 'Âge' avec la moyenne\n",
    "df['Age'] = df['Age'].fillna(df['Age'].mean())\n",
    "print(\"\\nDataFrame après gestion des valeurs manquantes:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47202c",
   "metadata": {},
   "source": [
    "\n",
    "**Transformer des types de données :**\n",
    "\n",
    "Supposons que la colonne \"Âge\" soit stockée sous forme de chaîne de caractères, nous pouvons la convertir en type `float` :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd77557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6 entries, 0 to 5\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Nom     6 non-null      object \n",
      " 1   Age     6 non-null      float64\n",
      " 2   Ville   6 non-null      object \n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 272.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58a297fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame après transformation des types:\n",
      "       Nom   Age    Ville\n",
      "0    Alice  25.0    Paris\n",
      "1      Bob  32.5  Londres\n",
      "2  Charlie  35.0   Berlin\n",
      "3      Bob  30.0  Londres\n",
      "4    Alice  32.5    Paris\n",
      "5    David  40.0   Berlin\n"
     ]
    }
   ],
   "source": [
    "# Transformer la colonne 'Âge' en type float\n",
    "df['Age'] = df['Age'].astype(\"float32\")\n",
    "print(\"\\nDataFrame après transformation des types:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06d159d",
   "metadata": {},
   "source": [
    "\n",
    "**Supprimer les doublons :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b7e5720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame après suppression des doublons:\n",
      "       Nom   Age    Ville\n",
      "0    Alice  25.0    Paris\n",
      "1      Bob  32.5  Londres\n",
      "2  Charlie  35.0   Berlin\n",
      "5    David  40.0   Berlin\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les doublons sur les colonnes 'Nom' et 'Ville'\n",
    "df_cleaned = df.drop_duplicates(subset=['Nom', 'Ville'])\n",
    "\n",
    "print(\"\\nDataFrame après suppression des doublons:\")\n",
    "print(df_cleaned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2235307",
   "metadata": {},
   "source": [
    "\n",
    "### **Résumé de la Partie 2 :**\n",
    "\n",
    "Dans cette partie, nous avons appris comment :\n",
    "\n",
    "1. **Gérer les valeurs manquantes** à l'aide de `dropna()`, `fillna()`, et `isna()`.\n",
    "2. **Transformer les types de données** avec la méthode `astype()`.\n",
    "3. **Supprimer les doublons** à l'aide de `drop_duplicates()`.\n",
    "\n",
    "Ces techniques de nettoyage des données sont fondamentales avant d'appliquer des transformations plus complexes et d'effectuer des analyses sur vos jeux de données.\n",
    "\n",
    "### Partie 2.2 : Transformation Avancée \n",
    "\n",
    "#### **1. Application de Fonctions sur les DataFrames avec `.apply()`, `.map()`, `.applymap()`**\n",
    "\n",
    "Pandas offre plusieurs méthodes puissantes pour appliquer des fonctions aux DataFrames. Ces méthodes permettent d’effectuer des transformations efficaces sur les données tout en conservant la structure du DataFrame.\n",
    "\n",
    "##### **`.apply()`**\n",
    "\n",
    "La méthode `.apply()` permet d’appliquer une fonction sur une ligne ou une colonne entière d’un DataFrame. Elle peut être utilisée sur des DataFrames ou des Series.\n",
    "\n",
    "- **Sur une colonne :** Vous pouvez appliquer une fonction à chaque élément d’une colonne.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45110a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Age  Age_carre\n",
      "0    Alice   25        625\n",
      "1      Bob   30        900\n",
      "2  Charlie   35       1225\n"
     ]
    }
   ],
   "source": [
    "  import pandas as pd\n",
    "  \n",
    "  # Exemple DataFrame\n",
    "  df = pd.DataFrame({\n",
    "      'Nom': ['Alice', 'Bob', 'Charlie'],\n",
    "      'Age': [25, 30, 35]\n",
    "  })\n",
    "  \n",
    "  # Appliquer une fonction à une colonne\n",
    "  df['Age_carre'] = df['Age'].apply(lambda line: line ** 2)\n",
    "  \n",
    "  print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d86c1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_carre</th>\n",
       "      <th>Age_triple</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>625</td>\n",
       "      <td>15625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>900</td>\n",
       "      <td>27000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>1225</td>\n",
       "      <td>42875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom  Age  Age_carre  Age_triple\n",
       "0    Alice   25        625       15625\n",
       "1      Bob   30        900       27000\n",
       "2  Charlie   35       1225       42875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def carre(x : int) -> int:\n",
    "    return x**3\n",
    "df[\"Age_triple\"]= df.Age.apply(carre)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ce005",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Sur une ligne :** Vous pouvez appliquer une fonction à chaque ligne du DataFrame en utilisant `axis=1`.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "744f111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Age  Age_carre  Age_triple        Nom_et_Age\n",
      "0    Alice   25        625       15625    Alice a 25 ans\n",
      "1      Bob   30        900       27000      Bob a 30 ans\n",
      "2  Charlie   35       1225       42875  Charlie a 35 ans\n"
     ]
    }
   ],
   "source": [
    "  # Appliquer une fonction sur chaque ligne\n",
    "  df['Nom_et_Age'] = df.apply(lambda row: f\"{row['Nom']} a {row['Age']} ans\", axis=1)\n",
    "  \n",
    "  print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b0d18",
   "metadata": {},
   "source": [
    "\n",
    "##### **`.map()`**\n",
    "\n",
    "La méthode `.map()` est plus spécifique pour les Series et permet de mapper des valeurs à partir de dictionnaires, de Series ou de fonctions. Elle est utile pour appliquer des transformations sur une colonne ou pour remplacer des valeurs spécifiques.\n",
    "\n",
    "- **Exemple de mappage avec un dictionnaire :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07c4491a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_carre</th>\n",
       "      <th>Age_triple</th>\n",
       "      <th>Nom_et_Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>625</td>\n",
       "      <td>15625</td>\n",
       "      <td>Alice a 25 ans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>900</td>\n",
       "      <td>27000</td>\n",
       "      <td>Bob a 30 ans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>1225</td>\n",
       "      <td>42875</td>\n",
       "      <td>Charlie a 35 ans</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom  Age  Age_carre  Age_triple        Nom_et_Age\n",
       "0    Alice   25        625       15625    Alice a 25 ans\n",
       "1      Bob   30        900       27000      Bob a 30 ans\n",
       "2  Charlie   35       1225       42875  Charlie a 35 ans"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b50e1c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Age  Age_carre  Age_triple        Nom_et_Age Nom_abbr\n",
      "0    Alice   25        625       15625    Alice a 25 ans        A\n",
      "1      Bob   30        900       27000      Bob a 30 ans        B\n",
      "2  Charlie   35       1225       42875  Charlie a 35 ans        C\n"
     ]
    }
   ],
   "source": [
    "  df['Nom_abbr'] = df['Nom'].map({'Alice': 'A', 'Bob': 'B', 'Charlie': 'C'})\n",
    "  print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e7defd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_carre</th>\n",
       "      <th>Age_triple</th>\n",
       "      <th>Nom_et_Age</th>\n",
       "      <th>Nom_abbr</th>\n",
       "      <th>Abbr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>625</td>\n",
       "      <td>15625</td>\n",
       "      <td>Alice a 25 ans</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>900</td>\n",
       "      <td>27000</td>\n",
       "      <td>Bob a 30 ans</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>1225</td>\n",
       "      <td>42875</td>\n",
       "      <td>Charlie a 35 ans</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom  Age  Age_carre  Age_triple        Nom_et_Age Nom_abbr Abbr\n",
       "0    Alice   25        625       15625    Alice a 25 ans        A    A\n",
       "1      Bob   30        900       27000      Bob a 30 ans        B    B\n",
       "2  Charlie   35       1225       42875  Charlie a 35 ans        C    C"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MapperNom(x : str)->str:\n",
    "    return x[0].upper()\n",
    "\n",
    "df[\"Abbr\"]= df.Nom.apply(MapperNom)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e6dcdb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- **Exemple d’utilisation d’une fonction avec `.map()` :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72b969dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_carre</th>\n",
       "      <th>Age_triple</th>\n",
       "      <th>Nom_et_Age</th>\n",
       "      <th>Nom_abbr</th>\n",
       "      <th>Abbr</th>\n",
       "      <th>Nom_upper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>625</td>\n",
       "      <td>15625</td>\n",
       "      <td>Alice a 25 ans</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>900</td>\n",
       "      <td>27000</td>\n",
       "      <td>Bob a 30 ans</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>1225</td>\n",
       "      <td>42875</td>\n",
       "      <td>Charlie a 35 ans</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom  Age  Age_carre  Age_triple        Nom_et_Age Nom_abbr Abbr  \\\n",
       "0    Alice   25        625       15625    Alice a 25 ans        A    A   \n",
       "1      Bob   30        900       27000      Bob a 30 ans        B    B   \n",
       "2  Charlie   35       1225       42875  Charlie a 35 ans        C    C   \n",
       "\n",
       "  Nom_upper  \n",
       "0         A  \n",
       "1         B  \n",
       "2         C  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  # Appliquer une fonction pour modifier les noms\n",
    "  df['Nom_upper'] = df['Nom'].map(MapperNom)\n",
    "  df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dadb91d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14176\\4040276167.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;31m# Appliquer une fonction sur chaque ligne\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Nom_et_Age'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"{row['Nom']} a {row['Age']} ans\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5985\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5986\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5987\u001b[0m         ):\n\u001b[0;32m   5988\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5989\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "  # faites attentention map pour les series # apply ( series et dataframe)\n",
    "  df['Nom_et_Age'] = df.map(lambda row: f\"{row['Nom']} a {row['Age']} ans\", axis=1)\n",
    "  \n",
    "  print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa001d5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### **`.applymap()`**\n",
    "\n",
    "La méthode `.applymap()` est utilisée pour appliquer une fonction à tous les éléments d’un DataFrame. Elle s’applique uniquement aux DataFrames (pas aux Series) et peut être utilisée pour effectuer des transformations de données sur l'ensemble du DataFrame.\n",
    "\n",
    "- **Exemple d’utilisation de `.applymap()` :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1082dacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A   B\n",
      "0  1  16\n",
      "1  4  25\n",
      "2  9  36\n"
     ]
    }
   ],
   "source": [
    "  df = pd.DataFrame({\n",
    "      'A': [1, 2, 3],\n",
    "      'B': [4, 5, 6]\n",
    "  })\n",
    "  \n",
    "  # Appliquer une fonction sur chaque élément du DataFrame\n",
    "  df = df.applymap(lambda x: x ** 2)\n",
    "  print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9f07e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Résumé des méthodes :**\n",
    "\n",
    "- **`.apply()`** : Applique une fonction sur une ligne ou une colonne d’un DataFrame ou une Series.\n",
    "- **`.map()`** : Permet de remplacer des valeurs dans une Series (très utile pour des remplacements ou des transformations).\n",
    "- **`.applymap()`** : Applique une fonction à tous les éléments d’un DataFrame (mais pas aux Series).\n",
    "\n",
    "#### **2. Transformation conditionnelle et ajout de nouvelles colonnes**\n",
    "\n",
    "L'ajout de nouvelles colonnes en fonction de conditions est une tâche courante dans l'analyse de données. Pandas permet de faire cela de manière simple avec des expressions conditionnelles.\n",
    "\n",
    "- **Ajout d'une nouvelle colonne avec une condition :** Vous pouvez ajouter une colonne en fonction des valeurs d'autres colonnes en utilisant des conditions.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "107fbe24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Age</th>\n",
       "      <th>Groupe_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>Jeune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>Mature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>Mature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom  Age Groupe_age\n",
       "0    Alice   25      Jeune\n",
       "1      Bob   30     Mature\n",
       "2  Charlie   35     Mature"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  # Ajouter une colonne 'Groupe_âge' basée sur l'âge\n",
    "  \n",
    "  # Exemple DataFrame\n",
    "  df = pd.DataFrame({\n",
    "      'Nom': ['Alice', 'Bob', 'Charlie'],\n",
    "      'Age': [25, 30, 35]\n",
    "  })\n",
    "  \n",
    "  df['Groupe_age'] = df['Age'].apply(lambda x: 'Jeune' if x < 30 else 'Mature')\n",
    "  df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a123d2",
   "metadata": {},
   "source": [
    "\n",
    "- **Ajouter une colonne avec des conditions multiples (utilisation de `np.select()`)** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b674406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Age</th>\n",
       "      <th>Groupe_age</th>\n",
       "      <th>Groupe_age_avance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25</td>\n",
       "      <td>Jeune</td>\n",
       "      <td>Jeune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>30</td>\n",
       "      <td>Mature</td>\n",
       "      <td>Adulte</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35</td>\n",
       "      <td>Mature</td>\n",
       "      <td>Adulte</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom  Age Groupe_age Groupe_age_avance\n",
       "0    Alice   25      Jeune             Jeune\n",
       "1      Bob   30     Mature            Adulte\n",
       "2  Charlie   35     Mature            Adulte"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  import numpy as np\n",
    "  \n",
    "  conditions = [\n",
    "      (df['Age'] < 30),\n",
    "      (df['Age'] >= 30) & (df['Age'] < 40),\n",
    "      (df['Age'] >= 40)\n",
    "  ]\n",
    "  \n",
    "  choix = ['Jeune', 'Adulte', 'Senior']\n",
    "  \n",
    "  df['Groupe_age_avance'] = np.select(conditions, choix, default='Inconnu')\n",
    "  df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57a9ec3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **3. Utilisation de `pd.cut()` pour la segmentation de données**\n",
    "\n",
    "La fonction `pd.cut()` est utilisée pour segmenter et classer des données continues en catégories (par exemple, des groupes d'âge). Cela permet de créer des classes ou des intervalles à partir de données numériques continues.\n",
    "\n",
    "- **Exemple de segmentation des âges :**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a6cbd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Age  Tranche_age\n",
      "0   18  Moins de 20\n",
      "1   25        20-30\n",
      "2   35        30-40\n",
      "3   42   Plus de 40\n",
      "4   50   Plus de 40\n",
      "5    1  Moins de 20\n",
      "6   15  Moins de 20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Importation de la bibliothèque pandas\n",
    "\n",
    "# Définir les bornes des intervalles (tranches d'âge)\n",
    "plages = [0, 20, 30, 40, 50]  \n",
    "# Les intervalles seront : \n",
    "# [0, 20[ (Moins de 20), [20, 30[ (20-30), [30, 40[ (30-40), [40, 50[ (Plus de 40)\n",
    "\n",
    "# Définir les étiquettes associées aux tranches d'âge\n",
    "choix = ['Moins de 20', '20-30', '30-40', 'Plus de 40']  \n",
    "# Ces étiquettes correspondent à chaque intervalle défini dans 'bins'\n",
    "\n",
    "# Exemple de DataFrame contenant des âges\n",
    "data = {'Age': [18, 25, 35, 42, 50, 1, 15]}  \n",
    "df = pd.DataFrame(data)  # Création de la DataFrame\n",
    "\n",
    "# Créer une nouvelle colonne 'Tranche_âge' en classant les âges dans les intervalles\n",
    "df['Tranche_age'] = pd.cut(\n",
    "    df['Age'],        # Colonne contenant les données numériques (ici les âges)\n",
    "    bins=plages,        # Les bornes des tranches\n",
    "    labels=choix,    # Les étiquettes pour les tranches\n",
    "    right=True       # Spécifie que les tranches sont ouvertes à droite (exclut la borne supérieure)\n",
    ")\n",
    "\n",
    "# Afficher la DataFrame avec la nouvelle colonne 'Tranche_âge'\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3d4a6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Résumé des transformations avancées :**\n",
    "\n",
    "1. **`.apply()`, `.map()`, `.applymap()`** sont des méthodes puissantes pour appliquer des fonctions à des DataFrames ou des Series. `.apply()` peut être utilisé sur des lignes ou des colonnes, `.map()` est plus spécifique aux Series, et `.applymap()` s'applique à tous les éléments d’un DataFrame.\n",
    "2. **Transformation conditionnelle** permet d'ajouter de nouvelles colonnes en fonction de conditions, utilisant des `lambda`, des opérateurs booléens, ou même `np.select()` pour des conditions multiples.\n",
    "3. **Segmentation avec `pd.cut()`** permet de diviser des données continues en catégories pour les transformer en classes, utiles dans des contextes comme les tranches d'âge.\n",
    "\n",
    "### **Exercice Pratique :**\n",
    "\n",
    "**Objectif :** Appliquer les transformations avancées à un DataFrame en utilisant\n",
    "\n",
    "Voici les solutions des exercices proposés dans la partie **2.2 Transformation Avancée**.\n",
    "\n",
    "**Exercice 1 : Créer un DataFrame avec des informations sur des individus**\n",
    "\n",
    "Nous allons créer un DataFrame avec des informations sur des individus, telles que leur nom, âge et salaire, puis appliquer les transformations demandées.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "569983d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame initial :\n",
      "        Nom  Age  Salaire\n",
      "0    Alice   25     3000\n",
      "1      Bob   32     4500\n",
      "2  Charlie   40     7000\n",
      "3    David   28     5000\n",
      "4      Eva   55     8000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Création du DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eva'],\n",
    "    'Age': [25, 32, 40, 28, 55],\n",
    "    'Salaire': [3000, 4500, 7000, 5000, 8000]\n",
    "})\n",
    "\n",
    "# Afficher le DataFrame initial\n",
    "print(\"DataFrame initial :\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e246d592",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Exercice 2 : Appliquer les transformations**\n",
    "\n",
    "#### **1. Créer une nouvelle colonne `Âge_groupe` qui classifie les individus en groupes d’âge.**\n",
    "\n",
    "Nous allons utiliser la fonction `pd.cut()` pour segmenter l'âge en groupes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f67c2622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame avec 'Âge_groupe' :\n",
      "        Nom  Age  Salaire   Age_groupe\n",
      "0    Alice   25     3000  Moins de 30\n",
      "1      Bob   32     4500        30-40\n",
      "2  Charlie   40     7000        40-50\n",
      "3    David   28     5000  Moins de 30\n",
      "4      Eva   55     8000   Plus de 50\n"
     ]
    }
   ],
   "source": [
    "# Définir les bins et les labels pour les groupes d'âge\n",
    "bins = [0, 30, 40, 50, 100]\n",
    "labels = ['Moins de 30', '30-40', '40-50', 'Plus de 50']\n",
    "\n",
    "# Créer la colonne 'Âge_groupe' en segmentant les âges\n",
    "df['Age_groupe'] = pd.cut(df['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Afficher le DataFrame après ajout de la colonne 'Âge_groupe'\n",
    "print(\"\\nDataFrame avec 'Âge_groupe' :\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccde1ea",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **2. Créer une colonne `Salaire_ajusté` qui applique une augmentation de salaire de 10% pour les personnes de plus de 30 ans.**\n",
    "\n",
    "Nous allons utiliser `.apply()` avec une fonction conditionnelle pour ajuster les salaires.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02a18073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame avec 'Salaire_ajusté' :\n",
      "        Nom  Age  Salaire   Age_groupe  Salaire_ajuste\n",
      "0    Alice   25     3000  Moins de 30          3000.0\n",
      "1      Bob   32     4500        30-40          4950.0\n",
      "2  Charlie   40     7000        40-50          7700.0\n",
      "3    David   28     5000  Moins de 30          5500.0\n",
      "4      Eva   55     8000   Plus de 50          8800.0\n"
     ]
    }
   ],
   "source": [
    "# Appliquer une augmentation de 10% pour les personnes de plus de 30 ans\n",
    "#df['Salaire_ajuste'] = df['Salaire'].apply(lambda x: x+x * 0.1 if x > 4000 else x)\n",
    "\n",
    "df['Salaire_ajuste'] = df['Salaire'].apply(lambda x: x * 1.1 if x > 4000 else x)\n",
    "\n",
    "# Afficher le DataFrame après ajout de la colonne 'Salaire_ajusté'\n",
    "print(\"\\nDataFrame avec 'Salaire_ajusté' :\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931b20c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **3. Utiliser `pd.cut()` pour segmenter les salaires en différentes catégories.**\n",
    "\n",
    "Nous allons créer des catégories de salaires à l'aide de `pd.cut()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6d7dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame avec 'Catégorie_salaire' :\n",
      "        Nom  Age  Salaire   Age_groupe  Salaire_ajuste Catégorie_salaire\n",
      "0    Alice   25     3000  Moins de 30          3000.0            Faible\n",
      "1      Bob   32     4500        30-40          4950.0           Moyenne\n",
      "2  Charlie   40     7000        40-50          7700.0            Élevée\n",
      "3    David   28     5000  Moins de 30          5500.0           Moyenne\n",
      "4      Eva   55     8000   Plus de 50          8800.0       Très élevée\n"
     ]
    }
   ],
   "source": [
    "# Définir les bins et les labels pour les salaires\n",
    "salary_bins = [0, 4000, 6000, 8000, 10000]\n",
    "salary_labels = ['Faible', 'Moyenne', 'Élevée', 'Très élevée']\n",
    "\n",
    "# Créer la colonne 'Catégorie_salaire' en segmentant les salaires\n",
    "df['Catégorie_salaire'] = pd.cut(df['Salaire'], bins=salary_bins, labels=salary_labels, right=False)\n",
    "\n",
    "# Afficher le DataFrame après ajout de la colonne 'Catégorie_salaire'\n",
    "print(\"\\nDataFrame avec 'Catégorie_salaire' :\\n\", df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da7387d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Résumé des solutions :**\n",
    "\n",
    "1. **`Âge_groupe`** : Utilisation de `pd.cut()` pour segmenter l'âge en groupes définis par des tranches.\n",
    "2. **`Salaire_ajusté`** : Application d'une condition sur le salaire pour augmenter de 10% ceux qui dépassent 4000.\n",
    "3. **`Catégorie_salaire`** : Utilisation de `pd.cut()` pour classer les salaires dans différentes catégories.\n",
    "\n",
    "Ces transformations sont très utiles pour la préparation des données avant une analyse plus approfondie ou pour les tâches de machine learning, car elles permettent de segmenter et de transformer les données en fonction de critères spécifiques.\n",
    "\n",
    "**Exercice pratique**  :\n",
    "\n",
    "- Appliquer     une fonction personnalisée sur un DataFrame.\n",
    "- Créer     de nouvelles colonnes basées sur des conditions logiques.\n",
    "\n",
    "### Partie 3 : Agrégation, GroupBy et Fusion des Données \n",
    "\n",
    "#### 3.1 GroupBy et Agrégation Avancée \n",
    "\n",
    "L’agrégation est une opération essentielle pour traiter les données par groupes et résumer des informations importantes. Pandas offre des fonctions puissantes pour effectuer des opérations de groupement et d’agrégation avec `groupby()`, qui permettent de diviser les données en groupes, d’appliquer des fonctions d'agrégation, et de combiner les résultats.\n",
    "\n",
    "\n",
    "### **Concepts clés** :\n",
    "\n",
    "- **GroupBy** : Permet de diviser un DataFrame ou une Series en sous-ensembles basés sur des critères spécifiques (ex : catégories, dates, etc.).\n",
    "- **Agrégation** : Applique des fonctions statistiques sur chaque groupe, comme `sum()`, `mean()`, `count()`, etc.\n",
    "- **Transformation après GroupBy** : Modifie les résultats de l'agrégation à l’aide de fonctions comme `transform()` et `filter()`.\n",
    "\n",
    "#### **1. Introduction à `groupby()` et Agrégation de Base **\n",
    "\n",
    "**Utilisation de `groupby()` avec des fonctions d'agrégation classiques** :\n",
    "\n",
    "Pandas fournit plusieurs fonctions d'agrégation communes, telles que `sum()`, `mean()`, `count()`, qui peuvent être utilisées après avoir appliqué `groupby()` sur un ou plusieurs critères.\n",
    "\n",
    "**Exemple** : On a un DataFrame avec des informations de ventes par région et produit. Nous allons calculer la somme des ventes par région.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a7a266f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region\n",
      "Est     750\n",
      "Nord    600\n",
      "Sud     600\n",
      "Name: Ventes, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création du DataFrame d'exemple\n",
    "data = {\n",
    "    'Region': ['Nord', 'Sud', 'Nord', 'Est', 'Sud', 'Est', 'Nord', 'Sud'],\n",
    "    'Produit': ['A', 'A', 'B', 'B', 'A', 'C', 'C', 'A'],\n",
    "    'Ventes': [200, 150, 300, 400, 250, 350, 100, 200]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Agrégation des ventes par région\n",
    "resultat = df.groupby('Region').Ventes.sum()\n",
    "\n",
    "# Affichage du résultat\n",
    "print(resultat.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc665a3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Dans cet exemple, nous avons utilisé `groupby('Région')` pour diviser les données en fonction de la région, puis nous avons utilisé la fonction `sum()` pour obtenir la somme des ventes par région.\n",
    "\n",
    "#### **2. Application de Fonctions Complexes sur les Groupes **\n",
    "\n",
    "Il est possible d'appliquer des fonctions plus complexes après avoir effectué un `groupby()`, comme l'utilisation de fonctions personnalisées ou d'opérations combinées.\n",
    "\n",
    "**Exemple** : Appliquons une fonction personnalisée qui calcule l'écart-type des ventes par région et produit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "658f93e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region  Produit\n",
      "Est     B           NaN\n",
      "        C           NaN\n",
      "Nord    A           NaN\n",
      "        B           NaN\n",
      "        C           NaN\n",
      "Sud     A          50.0\n",
      "Name: Ventes, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Fonction personnalisée pour calculer l'écart-type des ventes\n",
    "def ecart_type(x):\n",
    "    return x.std()\n",
    "\n",
    "# Application de la fonction après un GroupBy\n",
    "resultat_complexe = df.groupby(['Region', 'Produit'])['Ventes'].agg(ecart_type)\n",
    "\n",
    "# Affichage du résultat\n",
    "print(resultat_complexe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cff0d6",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ici, nous avons utilisé `agg()` pour appliquer la fonction `ecart_type()` qui calcule l'écart-type des ventes dans chaque groupe défini par la combinaison de la région et du produit.\n",
    "\n",
    "#### **3. Transformation après GroupBy avec `transform()` et `filter()` **\n",
    "\n",
    "Les méthodes `transform()` et `filter()` permettent d'effectuer des opérations de transformation ou de filtrage après avoir appliqué un `groupby()`.\n",
    "\n",
    "1. **`transform()`** : Permet de retourner un objet de la même forme que le DataFrame d'origine, avec des valeurs transformées par groupe.\n",
    "\n",
    "   Exemple d’utilisation de `transform()` pour calculer la moyenne des ventes par région, mais en retournant la moyenne pour chaque ligne :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "74b4453a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Region</th>\n",
       "      <th>Produit</th>\n",
       "      <th>Ventes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nord</td>\n",
       "      <td>A</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sud</td>\n",
       "      <td>A</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nord</td>\n",
       "      <td>B</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Est</td>\n",
       "      <td>B</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sud</td>\n",
       "      <td>A</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Region Produit  Ventes\n",
       "0   Nord       A     200\n",
       "1    Sud       A     150\n",
       "2   Nord       B     300\n",
       "3    Est       B     400\n",
       "4    Sud       A     250"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33078a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region Produit  Ventes  Ventes_moyennes\n",
      "0   Nord       A     200            200.0\n",
      "1    Sud       A     150            200.0\n",
      "2   Nord       B     300            200.0\n",
      "3    Est       B     400            375.0\n",
      "4    Sud       A     250            200.0\n",
      "5    Est       C     350            375.0\n",
      "6   Nord       C     100            200.0\n",
      "7    Sud       A     200            200.0\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de transform() pour appliquer une fonction à chaque groupe\n",
    "df['Ventes_moyennes'] = df.groupby('Region')['Ventes'].transform('mean')\n",
    "\n",
    "# Affichage du DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ee568aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Region\n",
       "Est     375.0\n",
       "Nord    200.0\n",
       "Sud     200.0\n",
       "Name: Ventes, dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('Region')['Ventes'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6695010f",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Dans cet exemple, chaque ligne du DataFrame a été enrichie avec la moyenne des ventes de la région correspondante grâce à `transform()`.\n",
    "\n",
    "1. **`filter()`** : Cette méthode permet de filtrer des groupes en fonction de critères sur les groupes eux-mêmes.\n",
    "\n",
    "Exemple pour garder uniquement les régions ayant plus de deux enregistrements :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3e6e8d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Region Produit  Ventes  Ventes_moyennes\n",
      "3    Est       B     400            375.0\n",
      "5    Est       C     350            375.0\n",
      "  Region Produit  Ventes  Ventes_moyennes\n",
      "0   Nord       A     200            200.0\n",
      "2   Nord       B     300            200.0\n",
      "6   Nord       C     100            200.0\n",
      "  Region Produit  Ventes  Ventes_moyennes\n",
      "1    Sud       A     150            200.0\n",
      "4    Sud       A     250            200.0\n",
      "7    Sud       A     200            200.0\n",
      "Empty DataFrame\n",
      "Columns: [Region, Produit, Ventes, Ventes_moyennes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Utilisation de filter() pour conserver uniquement les groupes ayant plus de 2 éléments\n",
    "df_filtered = df.groupby('Region').filter(lambda x: print(x))\n",
    "\n",
    "# Affichage du DataFrame filtré\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adfadb4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Dans cet exemple, seules les régions ayant plus de deux enregistrements ont été conservées dans le DataFrame final.\n",
    "\n",
    "### **Résumé des Concepts de GroupBy et Agrégation** :\n",
    "\n",
    "1. **`groupby()`** : Divise les données en groupes basés sur une ou plusieurs colonnes.\n",
    "2. **Fonctions d'agrégation** : `sum()`, `mean()`, `count()`, etc. permettent de calculer des statistiques par groupe.\n",
    "3. **Fonctions avancées** : Utilisation de `agg()` pour appliquer des fonctions personnalisées.\n",
    "4. **`transform()`** : Retourne un DataFrame avec des valeurs modifiées mais ayant la même structure.\n",
    "5. **`filter()`** : Filtrage des groupes selon des critères spécifiques.\n",
    "\n",
    "### **Exercice pratique  :**\n",
    "\n",
    "- **Objectif** : Appliquer un `groupby()` pour calculer des agrégations complexes et filtrer les données selon des conditions.\n",
    "\n",
    "**Consigne** :\n",
    "\n",
    "1. Créez un DataFrame contenant des informations sur des employés (nom, département, salaire, ancienneté).\n",
    "2. Calculez la moyenne des salaires par département.\n",
    "3. Utilisez `transform()` pour ajouter une colonne avec la moyenne des salaires par département.\n",
    "4. Utilisez `filter()` pour garder uniquement les départements ayant plus de 3 employés.\n",
    "\n",
    "**Solution attendue** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de099cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nom Departement  Salaire  Anciennete  Moyenne_Salaire_Dep\n",
      "3  David   Marketing     4000           8               3950.0\n",
      "7   Hugo   Marketing     3900           6               3950.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création du DataFrame d'exemple\n",
    "data_employes = {\n",
    "    'Nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Franck', 'Grace', 'Hugo'],\n",
    "    'Departement': ['IT', 'HR', 'IT', 'Marketing', 'HR', 'IT', 'HR', 'Marketing'],\n",
    "    'Salaire': [5000, 4500, 5500, 4000, 4300, 5100, 4200, 3900],\n",
    "    'Anciennete': [2, 5, 3, 8, 2, 4, 1, 6]\n",
    "}\n",
    "\n",
    "df_employes = pd.DataFrame(data_employes)\n",
    "\n",
    "# Moyenne des salaires par département\n",
    "moyenne_salaire = df_employes.groupby('Departement')['Salaire'].mean()\n",
    "\n",
    "# Ajouter une colonne de salaire moyen par département\n",
    "df_employes['Moyenne_Salaire_Dep'] = df_employes.groupby('Departement')['Salaire'].transform('mean')\n",
    "\n",
    "# Filtrer les départements ayant plus de 3 employés\n",
    "df_filtered = df_employes.groupby('Departement').filter(lambda x: len(x) <3)\n",
    "\n",
    "# Affichage du résultat final\n",
    "print(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb3df8",
   "metadata": {},
   "source": [
    "\n",
    "Ce type d'exercice permet aux étudiants de comprendre l'utilisation de `groupby()`, `transform()` et `filter()` pour effectuer des agrégations et filtrer des données de manière avancée.\n",
    "\n",
    "### Exercice Pratique  :\n",
    "\n",
    "**Objectif** :\n",
    "\n",
    "- Grouper des données par une colonne.\n",
    "- Appliquer des fonctions d'agrégation sur ces groupes.\n",
    "- Transformer les résultats à l'aide de `transform()`.\n",
    "\n",
    "### **Consignes de l'Exercice** :\n",
    "\n",
    "1. **Créer un DataFrame** avec les informations suivantes :\n",
    "   - `Nom` : Le nom de l'employé.\n",
    "   - `Département` : Le département auquel l'employé appartient.\n",
    "   - `Salaire` : Le salaire de l'employé.\n",
    "   - `Ancienneté` : L'ancienneté de l'employé dans l'entreprise.\n",
    "2. **Effectuer les étapes suivantes** :\n",
    "   - Grouper les employés par département.\n",
    "   - Calculer la moyenne des salaires et l'ancienneté totale dans chaque département.\n",
    "   - Ajouter une colonne dans le DataFrame d'origine, contenant la moyenne des salaires par département (en utilisant `transform()`).\n",
    "3. **Affichage des résultats** :\n",
    "   - Afficher le DataFrame après transformation, montrant les employés avec la moyenne des salaires par département.\n",
    "\n",
    "### *Solution attendue** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a6454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création du DataFrame d'exemple\n",
    "data_employes = {\n",
    "    'Nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Franck', 'Grace', 'Hugo'],\n",
    "    'Département': ['IT', 'HR', 'IT', 'Marketing', 'HR', 'IT', 'HR', 'Marketing'],\n",
    "    'Salaire': [5000, 4500, 5500, 4000, 4300, 5100, 4200, 3900],\n",
    "    'Ancienneté': [2, 5, 3, 8, 2, 4, 1, 6]\n",
    "}\n",
    "\n",
    "df_employes = pd.DataFrame(data_employes)\n",
    "\n",
    "# 1. Grouper les employés par département et calculer la moyenne des salaires\n",
    "moyenne_salaire = df_employes.groupby('Département')['Salaire'].mean()\n",
    "\n",
    "# 2. Grouper les employés par département et calculer l'ancienneté totale\n",
    "anciennete_totale = df_employes.groupby('Département')['Ancienneté'].sum()\n",
    "\n",
    "# 3. Utiliser transform() pour ajouter la moyenne des salaires par département dans le DataFrame d'origine\n",
    "df_employes['Moyenne_Salaire_Dep'] = df_employes.groupby('Département')['Salaire'].transform('mean')\n",
    "\n",
    "# 4. Afficher les résultats\n",
    "print(df_employes)\n",
    "\n",
    "# 5. Afficher la moyenne des salaires par département\n",
    "print(\"\\nMoyenne des salaires par département :\")\n",
    "print(moyenne_salaire)\n",
    "\n",
    "# 6. Afficher l'ancienneté totale par département\n",
    "print(\"\\nAncienneté totale par département :\")\n",
    "print(anciennete_totale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0d452",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Fusion et Jointure de DataFrames avec Pandas** \n",
    "\n",
    "#### Objectifs du cours :\n",
    "\n",
    "- **Fusionner des DataFrames avec `merge()`** : apprendre à joindre des tables sur des colonnes communes.\n",
    "- **Explorer les différents types de jointures** : `left`, `right`, `outer`, et `inner`.\n",
    "- **Utiliser `concat()`** : combiner des DataFrames horizontalement et verticalement.\n",
    "\n",
    "### 1. **Fusion de DataFrames avec `merge()` **\n",
    "\n",
    "La fonction `merge()` dans Pandas est utilisée pour fusionner deux DataFrames en fonction d’une ou plusieurs colonnes communes. Cette opération est similaire à une jointure SQL.\n",
    "\n",
    "#### Syntaxe de base :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b2f396",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = pd.merge(df1, df2, on='key', how='join_type')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6153d8df",
   "metadata": {},
   "source": [
    "\n",
    "- **`df1` et `df2`** : les deux DataFrames à fusionner.\n",
    "\n",
    "- **`on='key'`** : la colonne sur laquelle se fait la jointure. Elle doit exister dans les deux DataFrames.\n",
    "\n",
    "- `how='join_type'`\n",
    "\n",
    "   : définit le type de jointure à effectuer. Les options possibles sont :\n",
    "\n",
    "  - `'inner'` : jointure interne (par défaut).\n",
    "  - `'left'` : jointure à gauche.\n",
    "  - `'right'` : jointure à droite.\n",
    "  - `'outer'` : jointure externe.\n",
    "\n",
    "#### Types de jointures :\n",
    "\n",
    "- **Jointure interne (`inner`)** : Cette jointure garde seulement les lignes pour lesquelles les valeurs dans les colonnes communes sont présentes dans les deux DataFrames. C'est la jointure par défaut.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a06c97f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Nom  Salaire  Anciennete  department\n",
      "0    Alice     5000           2           5\n",
      "1      Bob     4500           5           2\n",
      "2  Charlie     5500           3           3\n",
      "3    David     4000           8           3\n",
      "4      Eva     4300           2           2\n",
      "5   Franck     5100           4           5\n",
      "6    Grace     4200           1           5\n",
      "7     Hugo     3900           6           3\n",
      "   Department        nom\n",
      "0           1         IT\n",
      "1           2         HR\n",
      "2           4  Marketing\n"
     ]
    }
   ],
   "source": [
    "data_employes = {\n",
    "    'Nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Franck', 'Grace', 'Hugo'],\n",
    "    'Salaire': [5000, 4500, 5500, 4000, 4300, 5100, 4200, 3900],\n",
    "    'Anciennete': [2, 5, 3, 8, 2, 4, 1, 6],\n",
    "    \"department\": [5, 2 ,3 , 3, 2,5, 5,3]\n",
    "}\n",
    "departement = {\n",
    "    'Department': [1, 2, 4],\n",
    "    'nom': ['IT', 'HR', 'Marketing'],\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data_employes)\n",
    "df2 = pd.DataFrame(departement)\n",
    "print(df1)\n",
    "print(df2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "499fc13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>Nom</th>\n",
       "      <th>Salaire</th>\n",
       "      <th>Anciennete</th>\n",
       "      <th>department_left</th>\n",
       "      <th>department_right</th>\n",
       "      <th>nom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>5</td>\n",
       "      <td>Alice</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5</td>\n",
       "      <td>Franck</td>\n",
       "      <td>5100.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6.0</th>\n",
       "      <td>5</td>\n",
       "      <td>Grace</td>\n",
       "      <td>4200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2</td>\n",
       "      <td>Bob</td>\n",
       "      <td>4500.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>2</td>\n",
       "      <td>Eva</td>\n",
       "      <td>4300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>3</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>5500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>3</td>\n",
       "      <td>David</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7.0</th>\n",
       "      <td>3</td>\n",
       "      <td>Hugo</td>\n",
       "      <td>3900.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     department      Nom  Salaire  Anciennete  department_left  \\\n",
       "0.0           5    Alice   5000.0         2.0              5.0   \n",
       "5.0           5   Franck   5100.0         4.0              5.0   \n",
       "6.0           5    Grace   4200.0         1.0              5.0   \n",
       "1.0           2      Bob   4500.0         5.0              2.0   \n",
       "4.0           2      Eva   4300.0         2.0              2.0   \n",
       "2.0           3  Charlie   5500.0         3.0              3.0   \n",
       "3.0           3    David   4000.0         8.0              3.0   \n",
       "7.0           3     Hugo   3900.0         6.0              3.0   \n",
       "NaN           0      NaN      NaN         NaN              NaN   \n",
       "NaN           1      NaN      NaN         NaN              NaN   \n",
       "\n",
       "     department_right        nom  \n",
       "0.0               NaN        NaN  \n",
       "5.0               NaN        NaN  \n",
       "6.0               NaN        NaN  \n",
       "1.0               4.0  Marketing  \n",
       "4.0               4.0  Marketing  \n",
       "2.0               NaN        NaN  \n",
       "3.0               NaN        NaN  \n",
       "7.0               NaN        NaN  \n",
       "NaN               1.0         IT  \n",
       "NaN               2.0         HR  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = df1.join(df2, on='department', rsuffix ='_right', lsuffix=\"_left\", how='outer')\n",
    "df_join.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ed8f19f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>department</th>\n",
       "      <th>nom</th>\n",
       "      <th>Nom</th>\n",
       "      <th>Salaire</th>\n",
       "      <th>Ancienneté</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Alice</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Franck</td>\n",
       "      <td>5100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Grace</td>\n",
       "      <td>4200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Alice</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Franck</td>\n",
       "      <td>5100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Grace</td>\n",
       "      <td>4200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Alice</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Franck</td>\n",
       "      <td>5100</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>IT</td>\n",
       "      <td>Grace</td>\n",
       "      <td>4200</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>Bob</td>\n",
       "      <td>4500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>Eva</td>\n",
       "      <td>4300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>Bob</td>\n",
       "      <td>4500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>Eva</td>\n",
       "      <td>4300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>Bob</td>\n",
       "      <td>4500</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>HR</td>\n",
       "      <td>Eva</td>\n",
       "      <td>4300</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>5500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>David</td>\n",
       "      <td>4000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Hugo</td>\n",
       "      <td>3900</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>5500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>David</td>\n",
       "      <td>4000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>Marketing</td>\n",
       "      <td>Hugo</td>\n",
       "      <td>3900</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    department        nom      Nom  Salaire  Ancienneté\n",
       "0            1         IT    Alice     5000           2\n",
       "1            1         IT   Franck     5100           4\n",
       "2            1         IT    Grace     4200           1\n",
       "3            1         IT    Alice     5000           2\n",
       "4            1         IT   Franck     5100           4\n",
       "5            1         IT    Grace     4200           1\n",
       "6            1         IT    Alice     5000           2\n",
       "7            1         IT   Franck     5100           4\n",
       "8            1         IT    Grace     4200           1\n",
       "9            2         HR      Bob     4500           5\n",
       "10           2         HR      Eva     4300           2\n",
       "11           2         HR      Bob     4500           5\n",
       "12           2         HR      Eva     4300           2\n",
       "13           2         HR      Bob     4500           5\n",
       "14           2         HR      Eva     4300           2\n",
       "15           3  Marketing  Charlie     5500           3\n",
       "16           3  Marketing    David     4000           8\n",
       "17           3  Marketing     Hugo     3900           6\n",
       "18           3  Marketing  Charlie     5500           3\n",
       "19           3  Marketing    David     4000           8\n",
       "20           3  Marketing     Hugo     3900           6"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "  df_inner = pd.merge(df2, df1, on='department', how='inner')\n",
    "  df_inner.head(50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde4e25",
   "metadata": {},
   "source": [
    "\n",
    "  Résultat : seules les lignes avec des `ID` présents dans les deux DataFrames seront conservées.\n",
    "\n",
    "- **Jointure à gauche (`left`)** : Cette jointure garde toutes les lignes du DataFrame de gauche (`df1`) et les lignes correspondantes du DataFrame de droite (`df2`). Si une ligne n’a pas de correspondance dans le DataFrame de droite, les valeurs correspondantes seront remplacées par `NaN`.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df_left = pd.merge(df1, df2, on='ID', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4385d33",
   "metadata": {},
   "source": [
    "\n",
    "  Résultat : toutes les lignes de `df1` seront conservées, et celles de `df2` seront fusionnées.\n",
    "\n",
    "- **Jointure à droite (`right`)** : Similaire à la jointure à gauche, mais cette fois, toutes les lignes du DataFrame de droite (`df2`) seront conservées, et celles du DataFrame de gauche (`df1`) seront fusionnées.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02158dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df_right = pd.merge(df1, df2, on='ID', how='right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c001f11",
   "metadata": {},
   "source": [
    "\n",
    "  Résultat : toutes les lignes de `df2` seront conservées.\n",
    "\n",
    "- **Jointure externe (`outer`)** : Cette jointure garde toutes les lignes des deux DataFrames. Les valeurs non correspondantes sont remplies avec `NaN`.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea4bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df_outer = pd.merge(df1, df2, on='ID', how='outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc253dde",
   "metadata": {},
   "source": [
    "\n",
    "  Résultat : toutes les lignes de `df1` et `df2` seront conservées, même celles qui ne se correspondent pas.\n",
    "\n",
    "### 2. **Concatenation de DataFrames avec `concat()` **\n",
    "\n",
    "La fonction `concat()` permet de combiner plusieurs DataFrames soit verticalement (en ajoutant des lignes) soit horizontalement (en ajoutant des colonnes).\n",
    "\n",
    "#### Syntaxe de base pour `concat()` :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4cbb31ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>Salaire</th>\n",
       "      <th>Anciennete</th>\n",
       "      <th>department</th>\n",
       "      <th>department</th>\n",
       "      <th>Nom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>5000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>4500</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>5500</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David</td>\n",
       "      <td>4000</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eva</td>\n",
       "      <td>4300</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Nom  Salaire  Anciennete  department  department        Nom\n",
       "0    Alice     5000           2           5         1.0         IT\n",
       "1      Bob     4500           5           2         2.0         HR\n",
       "2  Charlie     5500           3           3         4.0  Marketing\n",
       "3    David     4000           8           3         NaN        NaN\n",
       "4      Eva     4300           2           2         NaN        NaN"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_employes = {\n",
    "    'Nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eva', 'Franck', 'Grace', 'Hugo'],\n",
    "    'Salaire': [5000, 4500, 5500, 4000, 4300, 5100, 4200, 3900],\n",
    "    'Anciennete': [2, 5, 3, 8, 2, 4, 1, 6],\n",
    "    \"department\": [5, 2 ,3 , 3, 2,5, 5,3]\n",
    "}\n",
    "departement = {\n",
    "    'department': [1, 2, 4],\n",
    "    'Nom': ['IT', 'HR', 'Marketing'],\n",
    "}\n",
    "\n",
    "df1 = pd.DataFrame(data_employes)\n",
    "df2 = pd.DataFrame(departement)\n",
    "\n",
    "\n",
    "\n",
    "df_concat = pd.concat([df1, df2], axis=1, ignore_index=False)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c404cda5",
   "metadata": {},
   "source": [
    "\n",
    "- **`axis=0`** : concaténation verticale (ajouter des lignes).\n",
    "- **`axis=1`** : concaténation horizontale (ajouter des colonnes).\n",
    "- **`ignore_index=True`** : réindexe le DataFrame résultant (si `False`, les indices originaux des DataFrames sont conservés).\n",
    "\n",
    "#### Exemple de concaténation verticale :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756b8136",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Nom': ['Alice', 'Bob'], 'Âge': [25, 30]})\n",
    "df2 = pd.DataFrame({'Nom': ['Charlie', 'David'], 'Âge': [35, 40]})\n",
    "\n",
    "df_concat_vertical = pd.concat([df1, df2], axis=0, ignore_index=True)\n",
    "print(df_concat_vertical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60523fc4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Exemple de concaténation horizontale :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a343e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'Nom': ['Alice', 'Bob'], 'Âge': [25, 30]})\n",
    "df2 = pd.DataFrame({'Salaire': [50000, 60000], 'Département': ['IT', 'HR']})\n",
    "\n",
    "df_concat_horizontal = pd.concat([df1, df2], axis=1)\n",
    "print(df_concat_horizontal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132847a3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Exercice Pratique : Fusionner deux DataFrames sur une colonne clé \n",
    "\n",
    "**Objectif** : Fusionner deux DataFrames en utilisant différentes jointures et explorer les résultats.\n",
    "\n",
    "#### Étapes :\n",
    "\n",
    "1. Créez deux DataFrames :\n",
    "   - `df_employes` : contient des informations sur les employés, avec des colonnes `ID` et `Nom`.\n",
    "   - `df_departements` : contient des informations sur les départements, avec des colonnes `ID` et `Département`.\n",
    "2. Fusionnez les DataFrames avec les types de jointure suivants :\n",
    "   - `inner` : jointure interne.\n",
    "   - `left` : jointure à gauche.\n",
    "   - `right` : jointure à droite.\n",
    "   - `outer` : jointure externe.\n",
    "3. Affichez les résultats de chaque fusion pour observer les différences.\n",
    "\n",
    "#### Code de l'exercice :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Création des DataFrames\n",
    "df_employes = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4, 5],\n",
    "    'Nom': ['Alice', 'Bob', 'Charlie', 'David', 'Eva']\n",
    "})\n",
    "\n",
    "df_departements = pd.DataFrame({\n",
    "    'ID': [1, 2, 4, 5, 6],\n",
    "    'Département': ['IT', 'HR', 'Marketing', 'Finance', 'IT']\n",
    "})\n",
    "\n",
    "# 1. Fusion avec jointure interne\n",
    "df_inner = pd.merge(df_employes, df_departements, on='ID', how='inner')\n",
    "print(\"Jointure interne :\\n\", df_inner)\n",
    "\n",
    "# 2. Fusion avec jointure à gauche\n",
    "df_left = pd.merge(df_employes, df_departements, on='ID', how='left')\n",
    "print(\"\\nJointure à gauche :\\n\", df_left)\n",
    "\n",
    "# 3. Fusion avec jointure à droite\n",
    "df_right = pd.merge(df_employes, df_departements, on='ID', how='right')\n",
    "print(\"\\nJointure à droite :\\n\", df_right)\n",
    "\n",
    "# 4. Fusion avec jointure externe\n",
    "df_outer = pd.merge(df_employes, df_departements, on='ID', how='outer')\n",
    "print(\"\\nJointure externe :\\n\", df_outer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d60fc",
   "metadata": {},
   "source": [
    "\n",
    "#### Résultats attendus :\n",
    "\n",
    "- **Jointure interne** : Conserve les lignes où les `ID` existent dans les deux DataFrames.\n",
    "- **Jointure à gauche** : Conserve toutes les lignes de `df_employes`, avec les informations de `df_departements` où disponibles.\n",
    "- **Jointure à droite** : Conserve toutes les lignes de `df_departements`, avec les informations de `df_employes` où disponibles.\n",
    "- **Jointure externe** : Conserve toutes les lignes des deux DataFrames, avec des valeurs `NaN` pour les lignes sans correspondance.\n",
    "\n",
    "### Conclusion :\n",
    "\n",
    "- **`merge()`** est une fonction puissante pour effectuer des jointures SQL-like entre DataFrames.\n",
    "- **`concat()`** est utile pour combiner des DataFrames de manière horizontale ou verticale.\n",
    "- **Exercice pratique** permet de bien saisir l'impact des différents types de jointures dans la fusion des données.\n",
    "\n",
    "### **Optimisation des Performances avec Pandas et NumPy** \n",
    "\n",
    "#### Objectifs du cours :\n",
    "\n",
    "- **Optimisation de la mémoire** : réduire l'empreinte mémoire des DataFrames en utilisant des types de données efficaces et des techniques de chargement partiel.\n",
    "- **Meilleures pratiques pour les performances** : maximiser l'efficacité des opérations en utilisant Pandas et NumPy.\n",
    "\n",
    "### 4.1 **Optimisation de la Mémoire avec Pandas et NumPy** \n",
    "\n",
    "Lorsque vous travaillez avec de grandes quantités de données, l'optimisation de l'utilisation de la mémoire devient cruciale pour maintenir des performances élevées. Voici quelques techniques d'optimisation que vous pouvez utiliser.\n",
    "\n",
    "#### 1. **Types de données efficaces **\n",
    "\n",
    "Pandas permet de définir des types de données pour chaque colonne d’un DataFrame. Certains types de données prennent moins de mémoire que d’autres, et en choisissant les types appropriés, vous pouvez réduire considérablement l'empreinte mémoire de vos DataFrames.\n",
    "\n",
    "##### Types de données recommandés :\n",
    "\n",
    "- **`category`** : Si vous avez une colonne avec un nombre limité de catégories (par exemple, des valeurs comme \"Homme\" et \"Femme\" ou \"Paris\" et \"Lyon\"), vous pouvez convertir cette colonne en type `category` pour réduire l'utilisation mémoire. Ce type est beaucoup plus efficace que les types `object` ou `string`.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d5601",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df['Genre'] = df['Genre'].astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "aae1f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from skimage import data, img_as_float\n",
    "import matplotlib.pyplot as  plt\n",
    "\n",
    "# The original image is inverted as the object must be white.\n",
    "image = data.horse()\n",
    "\n",
    "plt.imshow(image, cmap='gray')\n",
    "print(image.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ade57a3",
   "metadata": {},
   "source": [
    "\n",
    "  Cela permet de réduire la mémoire de la colonne en utilisant des indices internes au lieu de stocker chaque valeur en tant que chaîne de caractères.\n",
    "\n",
    "- **`float32`** et **`int32`** : Les types par défaut pour les colonnes numériques sont souvent `float64` et `int64`, qui consomment beaucoup de mémoire. Vous pouvez les réduire à `float32` ou `int32` si les valeurs sont suffisamment petites. Cela est particulièrement utile si vous travaillez avec des datasets volumineux.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e0c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df['Prix'] = df['Prix'].astype('float32')\n",
    "  df['Quantité'] = df['Quantité'].astype('int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52c2dff",
   "metadata": {},
   "source": [
    "\n",
    "- **Utiliser `datetime` pour les dates** : Par défaut, les dates peuvent être stockées sous forme de chaînes de caractères (type `object`), mais il est plus efficace de les convertir en type `datetime` de Pandas. Cela améliore les performances lors de l'extraction et de la manipulation des dates.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80515715",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df['Date'] = pd.to_datetime(df['Date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc72d4d",
   "metadata": {},
   "source": [
    "\n",
    "##### Avantages des types optimisés :\n",
    "\n",
    "- **Réduction de la mémoire** : Vous pouvez réduire l'empreinte mémoire de vos DataFrames en choisissant des types plus compacts.\n",
    "- **Amélioration des performances** : Certaines opérations deviennent plus rapides lorsque les types sont optimisés (comme les opérations sur des `int32` vs `int64`).\n",
    "\n",
    "#### 2. **Chargement partiel des données avec `chunksize` **\n",
    "\n",
    "Si vous devez travailler avec un fichier de données volumineux (par exemple, un fichier CSV ou un fichier Excel), vous pouvez utiliser l’option `chunksize` de Pandas pour charger le fichier par morceaux. Cela permet de traiter les données en petits segments sans les charger entièrement en mémoire.\n",
    "\n",
    "##### Exemple avec `read_csv()` et `chunksize` :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3eb5dac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nom  prenom  age\n",
      "0  said 1   xx 1   59\n",
      "1  said 2   xx 2   32\n",
      "2  said 3   xx 3   50\n",
      "3  said 4   xx 4   28\n",
      "4  said 5   xx 5   74\n",
      "      Nom  prenom  age\n",
      "5   said 6   xx 6   13\n",
      "6   said 7   xx 7   44\n",
      "7   said 8   xx 8   48\n",
      "8   said 9   xx 9   68\n",
      "9  said 10  xx 10   59\n",
      "       Nom  prenom  age\n",
      "10  said 11  xx 11   46\n",
      "11  said 12  xx 12    4\n",
      "12  said 13  xx 13   35\n",
      "13  said 14  xx 14   30\n",
      "14  said 15  xx 15   53\n",
      "       Nom  prenom  age\n",
      "15  said 16  xx 16   62\n",
      "16  said 17  xx 17   74\n",
      "17  said 18  xx 18   66\n",
      "18  said 19  xx 19   66\n",
      "19  said 20  xx 20   15\n",
      "       Nom  prenom  age\n",
      "20  said 21  xx 21   71\n",
      "21  said 22  xx 22   70\n",
      "22  said 23  xx 23   33\n",
      "23  said 24  xx 24   34\n",
      "24  said 25  xx 25   25\n",
      "       Nom  prenom  age\n",
      "25  said 26  xx 26   17\n",
      "26  said 27  xx 27   78\n",
      "27  said 28  xx 28   23\n",
      "28  said 29  xx 29   24\n",
      "29  said 30  xx 30   64\n",
      "       Nom  prenom  age\n",
      "30  said 31  xx 31   37\n",
      "31  said 32  xx 32    8\n",
      "32  said 33  xx 33   78\n",
      "33  said 34  xx 34   78\n",
      "34  said 35  xx 35    1\n",
      "       Nom  prenom  age\n",
      "35  said 36  xx 36   73\n",
      "36  said 37  xx 37   43\n",
      "37  said 38  xx 38    4\n",
      "38  said 39  xx 39   57\n",
      "39  said 40  xx 40   60\n",
      "       Nom  prenom  age\n",
      "40  said 41  xx 41   13\n",
      "41  said 42  xx 42    2\n",
      "42  said 43  xx 43   37\n",
      "43  said 44  xx 44   68\n",
      "44  said 45  xx 45   60\n",
      "       Nom  prenom  age\n",
      "45  said 46  xx 46   22\n",
      "46  said 47  xx 47    1\n",
      "47  said 48  xx 48   28\n",
      "48  said 49  xx 49   52\n",
      "49  said 50  xx 50   58\n",
      "       Nom  prenom  age\n",
      "50  said 51  xx 51   32\n",
      "51  said 52  xx 52   80\n",
      "52  said 53  xx 53   79\n",
      "53  said 54  xx 54   57\n",
      "54  said 55  xx 55   72\n",
      "       Nom  prenom  age\n",
      "55  said 56  xx 56   58\n",
      "56  said 57  xx 57   28\n",
      "57  said 58  xx 58   55\n",
      "58  said 59  xx 59   43\n",
      "59  said 60  xx 60   61\n",
      "       Nom  prenom  age\n",
      "60  said 61  xx 61   76\n",
      "61  said 62  xx 62    9\n",
      "62  said 63  xx 63   75\n",
      "63  said 64  xx 64   51\n",
      "64  said 65  xx 65   34\n",
      "       Nom  prenom  age\n",
      "65  said 66  xx 66    5\n",
      "66  said 67  xx 67    6\n",
      "67  said 68  xx 68   61\n",
      "68  said 69  xx 69   44\n",
      "69  said 70  xx 70   15\n",
      "       Nom  prenom  age\n",
      "70  said 71  xx 71   40\n",
      "71  said 72  xx 72    3\n",
      "72  said 73  xx 73   17\n",
      "73  said 74  xx 74    2\n",
      "74  said 75  xx 75   48\n",
      "       Nom  prenom  age\n",
      "75  said 76  xx 76   30\n",
      "76  said 77  xx 77   47\n",
      "77  said 78  xx 78   49\n",
      "78  said 79  xx 79   24\n",
      "79  said 80  xx 80   73\n",
      "       Nom  prenom  age\n",
      "80  said 81  xx 81   26\n",
      "81  said 82  xx 82   54\n",
      "82  said 83  xx 83   36\n",
      "83  said 84  xx 84   50\n",
      "84  said 85  xx 85   23\n",
      "       Nom  prenom  age\n",
      "85  said 86  xx 86   11\n",
      "86  said 87  xx 87   74\n",
      "87  said 88  xx 88   24\n",
      "88  said 89  xx 89   25\n",
      "89  said 90  xx 90   28\n",
      "       Nom  prenom  age\n",
      "90  said 91  xx 91   15\n",
      "91  said 92  xx 92   41\n",
      "92  said 93  xx 93   12\n",
      "93  said 94  xx 94   69\n",
      "94  said 95  xx 95   24\n",
      "        Nom   prenom  age\n",
      "95   said 96   xx 96    1\n",
      "96   said 97   xx 97    9\n",
      "97   said 98   xx 98   57\n",
      "98   said 99   xx 99   51\n",
      "99  said 100  xx 100   16\n",
      "         Nom   prenom  age\n",
      "100  said 101  xx 101   22\n",
      "101  said 102  xx 102   64\n",
      "102  said 103  xx 103   18\n",
      "103  said 104  xx 104   23\n",
      "104  said 105  xx 105   58\n",
      "         Nom   prenom  age\n",
      "105  said 106  xx 106   29\n",
      "106  said 107  xx 107   59\n",
      "107  said 108  xx 108   57\n",
      "108  said 109  xx 109   69\n",
      "109  said 110  xx 110   57\n",
      "         Nom   prenom  age\n",
      "110  said 111  xx 111   46\n",
      "111  said 112  xx 112   59\n",
      "112  said 113  xx 113   56\n",
      "113  said 114  xx 114   59\n",
      "114  said 115  xx 115   69\n",
      "         Nom   prenom  age\n",
      "115  said 116  xx 116   63\n",
      "116  said 117  xx 117    2\n",
      "117  said 118  xx 118   56\n",
      "118  said 119  xx 119   45\n",
      "119  said 120  xx 120   60\n",
      "         Nom   prenom  age\n",
      "120  said 121  xx 121   31\n",
      "121  said 122  xx 122   55\n",
      "122  said 123  xx 123   46\n",
      "123  said 124  xx 124   19\n",
      "124  said 125  xx 125   52\n",
      "         Nom   prenom  age\n",
      "125  said 126  xx 126   29\n",
      "126  said 127  xx 127   14\n",
      "127  said 128  xx 128   11\n",
      "128  said 129  xx 129   19\n",
      "129  said 130  xx 130   14\n",
      "         Nom   prenom  age\n",
      "130  said 131  xx 131   75\n",
      "131  said 132  xx 132   46\n",
      "132  said 133  xx 133   45\n",
      "133  said 134  xx 134   41\n",
      "134  said 135  xx 135   61\n",
      "         Nom   prenom  age\n",
      "135  said 136  xx 136   32\n",
      "136  said 137  xx 137   30\n",
      "137  said 138  xx 138   40\n",
      "138  said 139  xx 139   56\n",
      "139  said 140  xx 140   50\n",
      "         Nom   prenom  age\n",
      "140  said 141  xx 141   27\n",
      "141  said 142  xx 142    9\n",
      "142  said 143  xx 143   79\n",
      "143  said 144  xx 144   19\n",
      "144  said 145  xx 145   76\n",
      "         Nom   prenom  age\n",
      "145  said 146  xx 146   80\n",
      "146  said 147  xx 147    4\n",
      "147  said 148  xx 148   68\n",
      "148  said 149  xx 149   39\n",
      "149  said 150  xx 150   35\n",
      "         Nom   prenom  age\n",
      "150  said 151  xx 151   58\n",
      "151  said 152  xx 152   26\n",
      "152  said 153  xx 153   53\n",
      "153  said 154  xx 154   37\n",
      "154  said 155  xx 155   22\n",
      "         Nom   prenom  age\n",
      "155  said 156  xx 156   10\n",
      "156  said 157  xx 157   51\n",
      "157  said 158  xx 158    6\n",
      "158  said 159  xx 159    4\n",
      "159  said 160  xx 160   41\n",
      "         Nom   prenom  age\n",
      "160  said 161  xx 161    6\n",
      "161  said 162  xx 162   76\n",
      "162  said 163  xx 163   31\n",
      "163  said 164  xx 164    2\n",
      "164  said 165  xx 165   35\n",
      "         Nom   prenom  age\n",
      "165  said 166  xx 166    9\n",
      "166  said 167  xx 167   31\n",
      "167  said 168  xx 168   70\n",
      "168  said 169  xx 169   36\n",
      "169  said 170  xx 170   15\n",
      "         Nom   prenom  age\n",
      "170  said 171  xx 171   53\n",
      "171  said 172  xx 172   40\n",
      "172  said 173  xx 173   28\n",
      "173  said 174  xx 174   11\n",
      "174  said 175  xx 175   48\n",
      "         Nom   prenom  age\n",
      "175  said 176  xx 176    7\n",
      "176  said 177  xx 177   22\n",
      "177  said 178  xx 178   32\n",
      "178  said 179  xx 179   40\n",
      "179  said 180  xx 180   49\n",
      "         Nom   prenom  age\n",
      "180  said 181  xx 181   46\n",
      "181  said 182  xx 182   62\n",
      "182  said 183  xx 183   76\n",
      "183  said 184  xx 184   33\n",
      "184  said 185  xx 185    8\n",
      "         Nom   prenom  age\n",
      "185  said 186  xx 186   37\n",
      "186  said 187  xx 187   23\n",
      "187  said 188  xx 188   57\n",
      "188  said 189  xx 189   44\n",
      "189  said 190  xx 190   49\n",
      "         Nom   prenom  age\n",
      "190  said 191  xx 191   23\n",
      "191  said 192  xx 192   70\n",
      "192  said 193  xx 193   79\n",
      "193  said 194  xx 194   73\n",
      "194  said 195  xx 195   39\n",
      "         Nom   prenom  age\n",
      "195  said 196  xx 196   11\n",
      "196  said 197  xx 197   38\n",
      "197  said 198  xx 198   53\n",
      "198  said 199  xx 199   47\n",
      "199  said 200  xx 200   62\n",
      "         Nom   prenom  age\n",
      "200  said 201  xx 201   29\n",
      "201  said 202  xx 202   50\n",
      "202  said 203  xx 203   66\n",
      "203  said 204  xx 204   48\n",
      "204  said 205  xx 205   10\n",
      "         Nom   prenom  age\n",
      "205  said 206  xx 206   63\n",
      "206  said 207  xx 207   66\n",
      "207  said 208  xx 208   34\n",
      "208  said 209  xx 209   79\n",
      "209  said 210  xx 210   64\n",
      "         Nom   prenom  age\n",
      "210  said 211  xx 211   67\n",
      "211  said 212  xx 212   31\n",
      "212  said 213  xx 213   52\n",
      "213  said 214  xx 214   41\n",
      "214  said 215  xx 215   20\n",
      "         Nom   prenom  age\n",
      "215  said 216  xx 216   62\n",
      "216  said 217  xx 217   50\n",
      "217  said 218  xx 218   18\n",
      "218  said 219  xx 219   42\n",
      "219  said 220  xx 220   13\n",
      "         Nom   prenom  age\n",
      "220  said 221  xx 221   67\n",
      "221  said 222  xx 222   31\n",
      "222  said 223  xx 223   28\n",
      "223  said 224  xx 224   64\n",
      "224  said 225  xx 225   67\n",
      "         Nom   prenom  age\n",
      "225  said 226  xx 226    1\n",
      "226  said 227  xx 227   28\n",
      "227  said 228  xx 228   48\n",
      "228  said 229  xx 229   43\n",
      "229  said 230  xx 230   54\n",
      "         Nom   prenom  age\n",
      "230  said 231  xx 231   34\n",
      "231  said 232  xx 232   68\n",
      "232  said 233  xx 233    8\n",
      "233  said 234  xx 234   63\n",
      "234  said 235  xx 235   50\n",
      "         Nom   prenom  age\n",
      "235  said 236  xx 236   23\n",
      "236  said 237  xx 237    5\n",
      "237  said 238  xx 238   44\n",
      "238  said 239  xx 239   74\n",
      "239  said 240  xx 240   17\n",
      "         Nom   prenom  age\n",
      "240  said 241  xx 241   18\n",
      "241  said 242  xx 242   47\n",
      "242  said 243  xx 243   10\n",
      "243  said 244  xx 244    9\n",
      "244  said 245  xx 245   51\n",
      "         Nom   prenom  age\n",
      "245  said 246  xx 246    6\n",
      "246  said 247  xx 247   10\n",
      "247  said 248  xx 248    8\n",
      "248  said 249  xx 249   49\n",
      "249  said 250  xx 250   41\n",
      "         Nom   prenom  age\n",
      "250  said 251  xx 251   25\n",
      "251  said 252  xx 252   51\n",
      "252  said 253  xx 253   50\n",
      "253  said 254  xx 254   37\n",
      "254  said 255  xx 255   76\n",
      "         Nom   prenom  age\n",
      "255  said 256  xx 256   37\n",
      "256  said 257  xx 257    1\n",
      "257  said 258  xx 258   59\n",
      "258  said 259  xx 259   54\n",
      "259  said 260  xx 260   14\n",
      "         Nom   prenom  age\n",
      "260  said 261  xx 261   59\n",
      "261  said 262  xx 262   56\n",
      "262  said 263  xx 263   34\n",
      "263  said 264  xx 264   40\n",
      "264  said 265  xx 265   65\n",
      "         Nom   prenom  age\n",
      "265  said 266  xx 266   13\n",
      "266  said 267  xx 267   43\n",
      "267  said 268  xx 268   11\n",
      "268  said 269  xx 269   27\n",
      "269  said 270  xx 270    3\n",
      "         Nom   prenom  age\n",
      "270  said 271  xx 271   74\n",
      "271  said 272  xx 272   66\n",
      "272  said 273  xx 273   70\n",
      "273  said 274  xx 274   18\n",
      "274  said 275  xx 275    2\n",
      "         Nom   prenom  age\n",
      "275  said 276  xx 276   69\n",
      "276  said 277  xx 277   70\n",
      "277  said 278  xx 278   67\n",
      "278  said 279  xx 279   30\n",
      "279  said 280  xx 280   52\n",
      "         Nom   prenom  age\n",
      "280  said 281  xx 281   22\n",
      "281  said 282  xx 282   78\n",
      "282  said 283  xx 283   28\n",
      "283  said 284  xx 284   37\n",
      "284  said 285  xx 285   49\n",
      "         Nom   prenom  age\n",
      "285  said 286  xx 286   69\n",
      "286  said 287  xx 287    7\n",
      "287  said 288  xx 288   14\n",
      "288  said 289  xx 289   24\n",
      "289  said 290  xx 290   38\n",
      "         Nom   prenom  age\n",
      "290  said 291  xx 291    9\n",
      "291  said 292  xx 292   27\n",
      "292  said 293  xx 293   80\n",
      "293  said 294  xx 294   49\n",
      "294  said 295  xx 295   26\n",
      "         Nom   prenom  age\n",
      "295  said 296  xx 296   63\n",
      "296  said 297  xx 297   80\n",
      "297  said 298  xx 298   52\n",
      "298  said 299  xx 299   79\n",
      "299  said 300  xx 300   17\n",
      "         Nom   prenom  age\n",
      "300  said 301  xx 301   10\n",
      "301  said 302  xx 302   36\n",
      "302  said 303  xx 303   14\n",
      "303  said 304  xx 304   32\n",
      "304  said 305  xx 305    1\n",
      "         Nom   prenom  age\n",
      "305  said 306  xx 306   54\n",
      "306  said 307  xx 307   10\n",
      "307  said 308  xx 308   34\n",
      "308  said 309  xx 309   17\n",
      "309  said 310  xx 310   29\n",
      "         Nom   prenom  age\n",
      "310  said 311  xx 311   33\n",
      "311  said 312  xx 312   35\n",
      "312  said 313  xx 313   38\n",
      "313  said 314  xx 314   75\n",
      "314  said 315  xx 315   46\n",
      "         Nom   prenom  age\n",
      "315  said 316  xx 316   35\n",
      "316  said 317  xx 317   53\n",
      "317  said 318  xx 318   17\n",
      "318  said 319  xx 319   27\n",
      "319  said 320  xx 320   28\n",
      "         Nom   prenom  age\n",
      "320  said 321  xx 321   42\n",
      "321  said 322  xx 322   58\n",
      "322  said 323  xx 323   76\n",
      "323  said 324  xx 324   29\n",
      "324  said 325  xx 325   14\n",
      "         Nom   prenom  age\n",
      "325  said 326  xx 326   47\n",
      "326  said 327  xx 327   12\n",
      "327  said 328  xx 328    8\n",
      "328  said 329  xx 329   28\n",
      "329  said 330  xx 330   10\n",
      "         Nom   prenom  age\n",
      "330  said 331  xx 331   36\n",
      "331  said 332  xx 332   31\n",
      "332  said 333  xx 333   68\n",
      "333  said 334  xx 334    4\n",
      "334  said 335  xx 335   48\n",
      "         Nom   prenom  age\n",
      "335  said 336  xx 336   58\n",
      "336  said 337  xx 337   48\n",
      "337  said 338  xx 338   53\n",
      "338  said 339  xx 339    2\n",
      "339  said 340  xx 340   29\n",
      "         Nom   prenom  age\n",
      "340  said 341  xx 341   41\n",
      "341  said 342  xx 342   55\n",
      "342  said 343  xx 343   57\n",
      "343  said 344  xx 344   13\n",
      "344  said 345  xx 345   10\n",
      "         Nom   prenom  age\n",
      "345  said 346  xx 346   79\n",
      "346  said 347  xx 347   23\n",
      "347  said 348  xx 348   18\n",
      "348  said 349  xx 349   66\n",
      "349  said 350  xx 350   15\n",
      "         Nom   prenom  age\n",
      "350  said 351  xx 351   50\n",
      "351  said 352  xx 352   53\n",
      "352  said 353  xx 353   65\n",
      "353  said 354  xx 354    4\n",
      "354  said 355  xx 355   66\n",
      "         Nom   prenom  age\n",
      "355  said 356  xx 356   21\n",
      "356  said 357  xx 357   55\n",
      "357  said 358  xx 358    1\n",
      "358  said 359  xx 359    5\n",
      "359  said 360  xx 360    3\n",
      "         Nom   prenom  age\n",
      "360  said 361  xx 361   52\n",
      "361  said 362  xx 362   57\n",
      "362  said 363  xx 363   49\n",
      "363  said 364  xx 364   37\n",
      "364  said 365  xx 365   77\n",
      "         Nom   prenom  age\n",
      "365  said 366  xx 366   22\n",
      "366  said 367  xx 367   48\n",
      "367  said 368  xx 368   21\n",
      "368  said 369  xx 369   56\n",
      "369  said 370  xx 370   74\n",
      "         Nom   prenom  age\n",
      "370  said 371  xx 371   52\n",
      "371  said 372  xx 372   56\n",
      "372  said 373  xx 373   56\n",
      "373  said 374  xx 374   26\n",
      "374  said 375  xx 375   43\n",
      "         Nom   prenom  age\n",
      "375  said 376  xx 376   50\n",
      "376  said 377  xx 377    6\n",
      "377  said 378  xx 378   42\n"
     ]
    }
   ],
   "source": [
    "chunk_size = 5  # Nombre de lignes par morceau\n",
    "chunks = pd.read_csv('data/volumes.csv', chunksize=chunk_size)\n",
    "\n",
    "for chunk in chunks:\n",
    "    # Traitez chaque morceau de données ici\n",
    "    print(chunk.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f9d1d",
   "metadata": {},
   "source": [
    "\n",
    "Cette technique permet de charger un fichier volumineux en mémoire et de le traiter morceau par morceau. Vous pouvez effectuer des opérations sur chaque \"chunk\" sans jamais charger le fichier entier dans la mémoire.\n",
    "\n",
    "##### Avantages de `chunksize` :\n",
    "\n",
    "- **Réduction de l’utilisation mémoire** : Vous ne chargez qu’une partie des données à la fois, ce qui permet de traiter des fichiers volumineux.\n",
    "- **Optimisation du traitement** : Il est possible de traiter un fichier par morceaux, en effectuant des agrégations ou des filtrages sur chaque morceau avant de passer au suivant.\n",
    "\n",
    "#### 3. **Optimisation mémoire avec `astype()` et `Categorical` (5 minutes)**\n",
    "\n",
    "##### Exemple d’utilisation de `astype()` et `Categorical` :\n",
    "\n",
    "Lors de l’importation de vos données, vous pouvez immédiatement appliquer une conversion de types de données pour réduire l’empreinte mémoire.\n",
    "\n",
    "1. **Réduction des types de données numériques** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "   df['Colonne_Numerique'] = df['Colonne_Numerique'].astype('float32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baeff0e",
   "metadata": {},
   "source": [
    "\n",
    "2. **Conversion de colonnes textuelles en `category`** : Si une colonne de texte contient une petite quantité de valeurs uniques, vous pouvez la convertir en type `category` pour réduire la mémoire utilisée.\n",
    "\n",
    "   Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae2c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "   df['Pays'] = df['Pays'].astype('category')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ab247",
   "metadata": {},
   "source": [
    "\n",
    "3. **Traitement des dates avec `datetime`** : Pour les colonnes de dates, utilisez `pd.to_datetime()` pour obtenir un type optimisé en mémoire.\n",
    "\n",
    "   Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "   df['Date'] = pd.to_datetime(df['Date'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608d8834",
   "metadata": {},
   "source": [
    "\n",
    "#### Résumé des optimisations de types :\n",
    "\n",
    "- Utiliser **`category`** pour les colonnes avec peu de valeurs uniques (par exemple, des catégories comme le sexe ou le pays).\n",
    "- Réduire les **types numériques** de `int64`/`float64` à `int32`/`float32` lorsque cela est possible.\n",
    "- Convertir les colonnes de **dates** au type `datetime`.\n",
    "\n",
    "### Conclusion : Optimisation des Performances avec Pandas et NumPy\n",
    "\n",
    "- **Optimisation de la mémoire** : L'utilisation de types de données appropriés est essentielle pour travailler efficacement avec de grandes quantités de données. L’optimisation de la mémoire permet de réduire l'empreinte des DataFrames et d’améliorer la vitesse des opérations.\n",
    "- **Chargement partiel des données** : Utiliser `chunksize` pour traiter des fichiers volumineux sans saturer la mémoire.\n",
    "- **Meilleures pratiques** : Choisir des types de données optimisés dès le départ et adapter le chargement des données en fonction de la taille de vos fichiers.\n",
    "\n",
    "Ces techniques sont particulièrement utiles dans le cadre de l’analyse de grands ensembles de données et permettent de travailler de manière plus fluide, même avec des ressources mémoire limitées.\n",
    "\n",
    "### Exercice Pratique : **Charger un gros fichier CSV par morceaux et optimiser la mémoire, filtrer et agréger les données**\n",
    "\n",
    "#### Objectifs :\n",
    "\n",
    "- **Charger un fichier CSV volumineux par morceaux** (utilisation de `chunksize`).\n",
    "- **Optimiser l'utilisation de la mémoire** pour chaque morceau en ajustant les types de données.\n",
    "- **Filtrer et agréger les données** dans chaque morceau avant de combiner les résultats.\n",
    "\n",
    "#### Étapes de l'Exercice :\n",
    "\n",
    "1. Charger le fichier CSV en morceaux à l'aide de `chunksize`.\n",
    "2. Optimiser les types de données dans chaque morceau pour réduire la mémoire utilisée.\n",
    "3. Appliquer des filtres sur les données pour ne conserver que les informations nécessaires.\n",
    "4. Effectuer une agrégation (par exemple, somme ou moyenne) sur un sous-ensemble de données.\n",
    "5. Combiner les résultats des morceaux si nécessaire.\n",
    "\n",
    "### Code de Solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ed8e2a4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>prenom</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said 1</td>\n",
       "      <td>xx 1</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>said 2</td>\n",
       "      <td>xx 2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>said 3</td>\n",
       "      <td>xx 3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>said 4</td>\n",
       "      <td>xx 4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>said 5</td>\n",
       "      <td>xx 5</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nom  prenom  age\n",
       "0  said 1   xx 1   59\n",
       "1  said 2   xx 2   32\n",
       "2  said 3   xx 3   50\n",
       "3  said 4   xx 4   28\n",
       "4  said 5   xx 5   74"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = pd.read_csv('data/volumes.csv')\n",
    "chunks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "09c4b800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nom  prenom  age\n",
      "0  said 1   xx 1   59\n",
      "1  said 2   xx 2   32\n",
      "2  said 3   xx 3   50\n",
      "3  said 4   xx 4   28\n",
      "4  said 5   xx 5   74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Définir la taille du morceau (chunksize) à 10 000 lignes\n",
    "chunk_size = 5\n",
    "\n",
    "# Créer un itérateur pour lire le fichier CSV par morceaux\n",
    "chunks = pd.read_csv('data/volumes.csv', chunksize=chunk_size)\n",
    "\n",
    "# Initialiser une liste pour stocker les résultats après traitement\n",
    "resultats = []\n",
    "\n",
    "# Traiter chaque morceau du fichier\n",
    "for chunk in chunks:\n",
    "    # Optimisation de la mémoire : convertir les types de données\n",
    "    # Exemple : 'Colonne_texte' devient 'category' si elle a peu de valeurs uniques\n",
    "    chunk['age'] = chunk['age'].astype('uint8')\n",
    "    # Convertir les colonnes numériques à un type plus léger (par exemple float32)\n",
    "    chunk['prenom'] = chunk['prenom'].astype('str')\n",
    "    chunk['Nom '] = chunk['Nom '].astype('str')\n",
    "    # Filtrage des données : par exemple, on garde uniquement les lignes où 'Colonne_numerique' > 10\n",
    "    chunk_filtre = chunk[chunk['age'] >= 18]\n",
    "    \n",
    "    # Agrégation : calculer la somme de 'Colonne_numerique' par 'Colonne_groupe'\n",
    "    #agregation = chunk_filtre.groupby('Nom ')['age'].sum()\n",
    "    \n",
    "    # Ajouter les résultats dans la liste\n",
    "    resultats.append(chunk_filtre)\n",
    "\n",
    "# Combiner les résultats des morceaux dans un DataFrame final\n",
    "df_final = pd.concat(resultats)\n",
    "\n",
    "# Afficher les premiers résultats\n",
    "print(df_final.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1948f84",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Suggestions d'Amélioration\n",
    "\n",
    "- **Sauvegarder les résultats** : Si le jeu de données est très volumineux, vous pouvez envisager de sauvegarder les résultats agrégés à chaque étape dans un fichier CSV ou une base de données, au lieu de les stocker uniquement en mémoire.\n",
    "\n",
    "  Exemple :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a846ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "  df_final.to_csv('resultats_agrégés.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b501a099",
   "metadata": {},
   "source": [
    "\n",
    "- **Optimisation supplémentaire avec `chunksize` plus petit ou plus grand** : Selon la taille du fichier, vous pouvez ajuster la taille du morceau pour trouver un compromis entre la vitesse et la mémoire disponible.\n",
    "\n",
    "\n",
    "### Utilisation de NumPy pour les Calculs Massifs \n",
    "\n",
    "#### Objectifs :\n",
    "\n",
    "- **Comprendre les tableaux NumPy** et leur rôle dans les calculs massifs.\n",
    "- **Maîtriser la vecteurisation des calculs** avec NumPy pour effectuer des opérations rapides sur des tableaux.\n",
    "- **Utiliser les fonctions universelles (ufuncs)** de NumPy pour appliquer des opérations vectorisées.\n",
    "\n",
    "### 1. **Introduction aux Tableaux NumPy **\n",
    "\n",
    "#### Qu'est-ce que NumPy ?\n",
    "\n",
    "NumPy est une bibliothèque Python puissante qui permet de manipuler des données sous forme de tableaux multidimensionnels (arrays). Ces tableaux sont plus efficaces et plus rapides que les listes Python classiques, surtout lorsqu'on travaille avec de grandes quantités de données numériques.\n",
    "\n",
    "#### Avantages des Tableaux NumPy :\n",
    "\n",
    "- **Vecteurisation** : NumPy permet de remplacer des boucles explicites (for) par des opérations vectorisées, ce qui rend les calculs beaucoup plus rapides.\n",
    "- **Performance** : NumPy est optimisé en C, ce qui permet de traiter de grands ensembles de données beaucoup plus rapidement que les listes Python natives.\n",
    "- **Facilité d'utilisation** : Les opérations arithmétiques sur les tableaux NumPy sont simples et intuitives.\n",
    "\n",
    "### 2. **Vecteurisation des Calculs avec NumPy **\n",
    "\n",
    "#### Vecteurisation des calculs :\n",
    "\n",
    "La **vecteurisation** fait référence à la capacité de NumPy à appliquer des opérations sur des tableaux entiers (vecteurs) sans avoir besoin de boucles explicites. Cela permet de traiter des calculs massifs de manière rapide et efficace.\n",
    "\n",
    "##### Exemple de Calcul Vecteurisé :\n",
    "\n",
    "Prenons deux tableaux NumPy et appliquons une opération élément par élément (addition dans cet exemple).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4f465b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Créer deux tableaux NumPy\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "b = np.array([5, 4, 3, 2, 1])\n",
    "\n",
    "# Additionner les tableaux\n",
    "resultat = a + b\n",
    "\n",
    "# Afficher le résultat\n",
    "print(resultat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1e5a1c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "somme=[]\n",
    "for i , k in zip(a , b):\n",
    "    somme.append(i+k)\n",
    "print(somme)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f1a6d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Ici, l'addition a été réalisée sur chaque élément des tableaux `a` et `b` de manière vectorisée, sans avoir besoin d'une boucle explicite.\n",
    "\n",
    "#### Autres opérations sur les tableaux NumPy :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "28074626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19 22]\n",
      " [43 50]]\n",
      "3.0\n"
     ]
    }
   ],
   "source": [
    "# Multiplication de matrices\n",
    "c = np.array([[1, 2], [3, 4]])\n",
    "d = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Produit matriciel\n",
    "produit = np.dot(c, d)\n",
    "print(produit)\n",
    "\n",
    "# Calcul de la moyenne\n",
    "moyenne = np.mean(a)\n",
    "print(moyenne)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe74924",
   "metadata": {},
   "source": [
    "\n",
    "### 3. **Fonctions Universelles (ufuncs) de NumPy **\n",
    "\n",
    "#### Qu'est-ce qu'une fonction universelle (ufunc) ?\n",
    "\n",
    "Les **fonctions universelles** ou **ufuncs** sont des fonctions qui permettent d'effectuer des opérations élément par élément sur des tableaux NumPy. Ces fonctions sont extrêmement rapides et efficaces car elles sont optimisées en C.\n",
    "\n",
    "##### Exemples de ufuncs :\n",
    "\n",
    "- **Addition, soustraction, multiplication, division**.\n",
    "- **Fonctions mathématiques** : `np.sin()`, `np.cos()`, `np.exp()`, etc.\n",
    "- **Fonctions statistiques** : `np.mean()`, `np.std()`, `np.sum()`, etc.\n",
    "\n",
    "##### Exemple d'ufunc sur un tableau NumPy :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fa8a98f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0000000e+00  1.0000000e+00  1.2246468e-16 -1.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# Créer un tableau NumPy\n",
    "x = np.array([0, np.pi/2, np.pi, 3*np.pi/2])\n",
    "\n",
    "# Appliquer une fonction trigonométrique (sin) sur tous les éléments du tableau\n",
    "sin_x = np.sin(x)\n",
    "print(sin_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4883f942",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### D'autres exemples de fonctions universelles de NumPy :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aa89dbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.           4.81047738  23.14069263 111.31777849]\n",
      "[0.         0.94421571 1.42108041 1.74263732]\n",
      "Sommation : 9.42477796076938, Moyenne : 2.356194490192345\n"
     ]
    }
   ],
   "source": [
    "# Exponentielle d'un tableau\n",
    "exp_x = np.exp(x)\n",
    "print(exp_x)\n",
    "\n",
    "# Calcul du logarithme naturel\n",
    "log_x = np.log(x + 1)  # Ajouter 1 pour éviter log(0)\n",
    "print(log_x)\n",
    "\n",
    "# Calcul de la somme et de la moyenne d'un tableau\n",
    "sum_x = np.sum(x)\n",
    "mean_x = np.mean(x)\n",
    "print(f\"Sommation : {sum_x}, Moyenne : {mean_x}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a083bd",
   "metadata": {},
   "source": [
    "\n",
    "### 4. **Application Avancée des ufuncs et de la Vecteurisation**\n",
    "\n",
    "#### Exemple d'application de calculs massifs avec NumPy :\n",
    "\n",
    "Imaginons que nous devons appliquer une fonction complexe à un tableau de 1 million de nombres, par exemple pour transformer des données ou faire des calculs sur de grandes séries temporelles.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0b6bcace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne : 0.38610976838443073, Écart-type : 0.1974371277369312\n"
     ]
    }
   ],
   "source": [
    "# Créer un tableau de 1 million de nombres\n",
    "data = np.random.rand(1000000)\n",
    "\n",
    "# Appliquer une transformation (ex : logarithme)\n",
    "transformation = np.log(data + 1)\n",
    "\n",
    "# Calculer des statistiques (moyenne et écart-type)\n",
    "moyenne = np.mean(transformation)\n",
    "ecart_type = np.std(transformation)\n",
    "\n",
    "print(f\"Moyenne : {moyenne}, Écart-type : {ecart_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d946d8e",
   "metadata": {},
   "source": [
    "\n",
    "Ce type d'approche vectorisée est bien plus rapide que l'utilisation de boucles `for` classiques en Python et permet de traiter des millions de données en quelques lignes de code.\n",
    "\n",
    "### Résumé des Concepts Abordés :\n",
    "\n",
    "- **Tableaux NumPy** : Ils sont fondamentaux pour les calculs massifs et la manipulation de données numériques.\n",
    "- **Vecteurisation** : C'est l'application d'opérations sur des tableaux entiers sans avoir besoin de boucles explicites.\n",
    "- **Fonctions Universelles (ufuncs)** : Ce sont des fonctions optimisées en C qui permettent des calculs rapides sur des tableaux.\n",
    "\n",
    "En utilisant ces techniques avancées de NumPy, vous pouvez manipuler et analyser de grandes quantités de données de manière efficace et performante.\n",
    "\n",
    "### Partie 4.2 : Utilisation de NumPy pour les Calculs Massifs \n",
    "\n",
    "#### Objectif :\n",
    "\n",
    "Cette section se concentre sur l’utilisation de **NumPy** pour effectuer des calculs massifs et optimiser les performances des traitements sur des jeux de données volumineux. Nous verrons comment tirer parti de la **vecteurisation**, des **fonctions universelles (ufuncs)**, et de l'optimisation par rapport aux boucles classiques en Python.\n",
    "\n",
    "### **1. Tableaux NumPy : Vecteurisation des Calculs**\n",
    "\n",
    "La **vecteurisation** est une technique qui consiste à appliquer des opérations sur des **tableaux complets** (ou des matrices) plutôt que sur chaque élément de manière individuelle. Cela permet d’optimiser les performances en réduisant le nombre d'itérations explicites dans le code.\n",
    "\n",
    "#### Exemple de vecteurisation avec NumPy\n",
    "\n",
    "Imaginons que nous voulions ajouter deux listes de 1 million d'éléments.\n",
    "\n",
    "##### Avec des **listes Python classiques** (sans vecteurisation) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "302b909b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps avec boucle classique : 0.1600644588470459 secondes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Création de deux listes Python\n",
    "a = [1] * 1000000\n",
    "b = [2] * 1000000\n",
    "\n",
    "# Démarrer le chronomètre\n",
    "start_time = time.time()\n",
    "\n",
    "# Addition des éléments dans une boucle\n",
    "result = []\n",
    "for i in range(len(a)):\n",
    "    result.append(a[i] + b[i])\n",
    "\n",
    "# Afficher le temps de calcul\n",
    "end_time = time.time()\n",
    "print(f\"Temps avec boucle classique : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d32fe0",
   "metadata": {},
   "source": [
    "\n",
    "##### Avec **NumPy** (vecteurisation) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "02355834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps avec NumPy : 0.006203413009643555 secondes\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Création de deux tableaux NumPy\n",
    "a = np.ones(1000000)\n",
    "b = np.ones(1000000) * 2\n",
    "\n",
    "# Démarrer le chronomètre\n",
    "start_time = time.time()\n",
    "\n",
    "# Addition des tableaux (vecteurisation)\n",
    "result = a + b\n",
    "\n",
    "# Afficher le temps de calcul\n",
    "end_time = time.time()\n",
    "print(f\"Temps avec NumPy : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7124288",
   "metadata": {},
   "source": [
    "\n",
    "**Pourquoi la vecteurisation est plus rapide ?**\n",
    "\n",
    "- En utilisant NumPy, l'opération `a + b` est effectuée sur l'ensemble des éléments des tableaux en une seule instruction au niveau du compilateur **C**, ce qui est bien plus rapide que de le faire élément par élément avec une boucle Python classique.\n",
    "\n",
    "### **2. Fonctions Universelles (ufuncs)**\n",
    "\n",
    "Les **ufuncs** (fonctions universelles) sont des fonctions NumPy qui permettent d'appliquer des opérations élémentaires à chaque élément d'un tableau de manière **vectorisée**. Elles sont très optimisées et sont exécutées beaucoup plus rapidement que des boucles classiques.\n",
    "\n",
    "#### Exemple avec `ufuncs` : Addition, Multiplication, et Fonction Sine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cc76862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition avec constante 10 :  [11 12 13 14 15]\n",
      "Multiplication par 2 :  [ 2  4  6  8 10]\n",
      "Sinus de chaque élément :  [ 0.84147098  0.90929743  0.14112001 -0.7568025  -0.95892427]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Créer un tableau NumPy\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Addition avec une constante\n",
    "result_add = np.add(arr, 10)\n",
    "\n",
    "# Multiplication avec une constante\n",
    "result_mul = np.multiply(arr, 2)\n",
    "\n",
    "# Appliquer la fonction sin sur chaque élément\n",
    "result_sin = np.sin(arr)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Addition avec constante 10 : \", result_add)\n",
    "print(\"Multiplication par 2 : \", result_mul)\n",
    "print(\"Sinus de chaque élément : \", result_sin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b99436",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **3. Optimisation des Calculs avec NumPy par Rapport aux Boucles Python Classiques**\n",
    "\n",
    "#### Exemple : Calcul du Carré d'un Tableau\n",
    "\n",
    "##### Avec des **boucles Python classiques** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Création d'un tableau Python\n",
    "a = [1, 2, 3, 4, 5]\n",
    "\n",
    "# Démarrer le chronomètre\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculer le carré de chaque élément avec une boucle\n",
    "result = []\n",
    "for x in a:\n",
    "    result.append(x ** 2)\n",
    "\n",
    "# Afficher le temps de calcul\n",
    "end_time = time.time()\n",
    "print(f\"Temps avec boucle classique : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984a3ca2",
   "metadata": {},
   "source": [
    "\n",
    "##### Avec **NumPy** (optimisation) :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5798c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Création d'un tableau NumPy\n",
    "a = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Démarrer le chronomètre\n",
    "start_time = time.time()\n",
    "\n",
    "# Calculer le carré de chaque élément avec NumPy\n",
    "result = np.square(a)\n",
    "\n",
    "# Afficher le temps de calcul\n",
    "end_time = time.time()\n",
    "print(f\"Temps avec NumPy : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdde822",
   "metadata": {},
   "source": [
    "\n",
    "#### Explication :\n",
    "\n",
    "- **Boucles Python classiques** : Chaque élément du tableau est traité un par un, et le calcul du carré de chaque élément nécessite une itération explicite.\n",
    "- **NumPy** : Le calcul est effectué **en une seule instruction** sur l'ensemble du tableau, grâce à la **vecteurisation**. Cela améliore considérablement la vitesse des calculs, particulièrement pour les grands ensembles de données.\n",
    "\n",
    "#### **Vitesse et Mémoire** :\n",
    "\n",
    "- **NumPy** permet une meilleure gestion de la mémoire, en stockant les données dans un format compact en **mémoire contiguë**, contrairement aux **listes Python**, qui sont constituées de pointeurs vers des objets.\n",
    "- **NumPy** optimise également l'**accès mémoire** en garantissant des calculs **plus rapides** grâce à son implémentation en **C**.\n",
    "\n",
    "### **Conclusion**\n",
    "\n",
    "- L'utilisation de **NumPy** pour les calculs massifs est un véritable gain de performance, grâce à la **vecteurisation** et aux **ufuncs**, qui permettent de traiter de grandes quantités de données en **un seul appel fonctionnel**.\n",
    "- Par rapport aux **boucles Python classiques**, NumPy permet de réduire considérablement le temps de traitement et la complexité du code, tout en optimisant la gestion de la mémoire.\n",
    "\n",
    "**Application concrète :** Si vous travaillez avec des jeux de données volumineux (par exemple, des séries chronologiques ou des calculs statistiques sur de grandes matrices), utiliser **NumPy** vous permettra de manipuler ces données beaucoup plus efficacement qu'avec des boucles Python classiques.\n",
    "\n",
    "### Exercice Pratique : Vecteurisation avec NumPy vs Boucles Python Classiques\n",
    "\n",
    "#### Objectif :\n",
    "\n",
    "Dans cet exercice, vous allez créer un tableau NumPy et appliquer des calculs mathématiques (comme l'addition, la multiplication, et l'exponentiation) de manière vectorisée. Ensuite, vous comparerez la performance de NumPy avec celle des boucles Python classiques pour effectuer ces mêmes calculs.\n",
    "\n",
    "### Étapes à suivre :\n",
    "\n",
    "1. **Créer un tableau NumPy** de 1 million d'éléments, initialisé à une valeur de votre choix (par exemple, 1.0).\n",
    "\n",
    "2. Appliquer des calculs\n",
    "\n",
    "    sur ce tableau en utilisant :\n",
    "\n",
    "   - **NumPy (vectorisation)**\n",
    "   - **Boucles Python classiques**\n",
    "\n",
    "3. **Comparer les performances** de NumPy et des boucles classiques en mesurant le temps d'exécution des deux approches.\n",
    "\n",
    "### Partie 1 : Créer un tableau NumPy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c736dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Créer un tableau NumPy de 1 million d'éléments (par exemple, tous initialisés à 1)\n",
    "a = np.ones(1000000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed177d25",
   "metadata": {},
   "source": [
    "\n",
    "### Partie 2 : Application des Calculs - **NumPy (Vecteurisation)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrer le chronomètre\n",
    "start_time = time.time()\n",
    "\n",
    "# Appliquer des calculs avec NumPy (addition, multiplication, et exponentiation)\n",
    "result_numpy = np.square(a) + np.multiply(a, 2) + np.exp(a)\n",
    "\n",
    "# Calculer le temps d'exécution\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'exécution avec NumPy : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e9773",
   "metadata": {},
   "source": [
    "\n",
    "### Partie 3 : Application des Calculs - **Boucles Python Classiques**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbeaab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Démarrer le chronomètre\n",
    "start_time = time.time()\n",
    "\n",
    "# Appliquer des calculs avec des boucles Python classiques\n",
    "result_python = []\n",
    "for x in a:\n",
    "    result_python.append(x ** 2 + x * 2 + np.exp(x))\n",
    "\n",
    "# Calculer le temps d'exécution\n",
    "end_time = time.time()\n",
    "print(f\"Temps d'exécution avec boucles Python classiques : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ebfac9",
   "metadata": {},
   "source": [
    "\n",
    "### Conclusion :\n",
    "\n",
    "Vous allez observer que l'approche **NumPy** (vecteurisation) est bien plus rapide, car elle évite les boucles explicites et exécute les opérations de manière optimisée en utilisant des fonctions basées sur **C** sous-jacentes. À l'inverse, les **boucles Python classiques** sont plus lentes car chaque élément doit être traité individuellement, ce qui nécessite davantage de temps de traitement.\n",
    "\n",
    "Cet exercice vous montre l'importance d'utiliser **NumPy** pour les opérations mathématiques sur de grandes quantités de données, en particulier dans les applications nécessitant des calculs complexes sur de larges ensembles de données.\n",
    "\n",
    "### Traitement Parallèle et Gestion de Gros Volumes de Données \n",
    "\n",
    "#### 5.1 Traitement Parallèle avec Dask\n",
    "\n",
    "##### **Introduction à Dask**\n",
    "\n",
    "Dask est une bibliothèque Python qui permet de paralléliser les calculs et de gérer efficacement de grands volumes de données, souvent plus grands que la mémoire disponible sur une machine individuelle. Dask est conçu pour être compatible avec Pandas, ce qui permet de manipuler des ensembles de données de manière transparente et de manière distribuée.\n",
    "\n",
    "##### **Parallélisation des opérations Pandas pour les gros volumes de données**\n",
    "\n",
    "Les opérations sur des jeux de données volumineux avec Pandas peuvent devenir lentes et inefficaces en raison des limitations de mémoire. Dask permet de diviser un grand ensemble de données en **blocs** ou **partitions** et d'effectuer les calculs sur ces partitions en parallèle, ce qui accélère de manière significative les opérations. Les utilisateurs peuvent travailler avec Dask de manière similaire à Pandas, mais avec la possibilité de traiter des données beaucoup plus grandes.\n",
    "\n",
    "Dask utilise des **DataFrames** et des **Arrays** qui sont similaires aux structures de Pandas et NumPy, mais qui sont distribués sur plusieurs threads ou machines. En utilisant Dask, les utilisateurs peuvent effectuer des opérations sur des ensembles de données plus volumineux que la mémoire, sans avoir à se soucier des limitations de mémoire.\n",
    "\n",
    "##### **Chargement et traitement parallèle de données avec Dask DataFrames**\n",
    "\n",
    "Un Dask DataFrame ressemble à un DataFrame Pandas, mais il est en réalité un ensemble de DataFrames plus petits appelés **partitions**. Dask charge les données de manière paresseuse, ce qui signifie que les opérations sont différées jusqu'à ce qu'elles soient réellement nécessaires (lorsque vous demandez un calcul comme `.compute()`).\n",
    "\n",
    "Voici un exemple d'utilisation de Dask pour charger et manipuler des données :\n",
    "\n",
    "1. **Installation de Dask**\n",
    "\n",
    "   Avant de commencer, assurez-vous d'avoir installé Dask via pip :\n",
    "\n",
    "   ```bash\n",
    "   pip install dask\n",
    "   ```\n",
    "\n",
    "2. **Chargement de données avec Dask** Supposons que vous avez un fichier CSV volumineux et vous souhaitez le charger et le manipuler à l'aide de Dask :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1c18023b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nom  prenom  age\n",
      "0  said 1   xx 1   59\n",
      "1  said 2   xx 2   32\n",
      "2  said 3   xx 3   50\n",
      "3  said 4   xx 4   28\n",
      "4  said 5   xx 5   74\n"
     ]
    }
   ],
   "source": [
    "   import dask.dataframe as dd\n",
    "   \n",
    "   # Charger un grand fichier CSV avec Dask\n",
    "   df = dd.read_csv('data/volumes.csv')\n",
    "   \n",
    "   # Afficher les premières lignes de données (similaire à Pandas)\n",
    "   print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a84a48d",
   "metadata": {},
   "source": [
    "\n",
    "   Ici, Dask lit le fichier en plusieurs partitions en parallèle, permettant ainsi une gestion efficace des fichiers volumineux.\n",
    "\n",
    "3. **Opérations sur les données** Dask fonctionne de manière similaire à Pandas pour les opérations de transformation et d'analyse des données. Par exemple, vous pouvez effectuer des opérations comme `groupby`, `agg`, et `join` :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "35e94a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nom \n",
      "said 1      59\n",
      "said 10     59\n",
      "said 100    16\n",
      "said 101    22\n",
      "said 102    64\n",
      "            ..\n",
      "said 95     24\n",
      "said 96      1\n",
      "said 97      9\n",
      "said 98     57\n",
      "said 99     51\n",
      "Name: age, Length: 378, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "   # Appliquer une agrégation sur les données\n",
    "   result = df.groupby('Nom ')['age'].sum()\n",
    "   \n",
    "   # Calculer le résultat\n",
    "   print(result.compute())  # .compute() pour exécuter réellement le calcul\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0ae64d",
   "metadata": {},
   "source": [
    "\n",
    "   Dans cet exemple, Dask effectue l'agrégation en parallèle sur les différentes partitions des données, ce qui permet de traiter de grands volumes de données rapidement.\n",
    "\n",
    "##### **Dask vs Pandas : Quand et pourquoi utiliser Dask**\n",
    "\n",
    "- Pandas:\n",
    "\n",
    "   - Idéal pour des jeux de données qui tiennent en mémoire.\n",
    "  - Très rapide pour des petits à moyens volumes de données.\n",
    "  - Opère sur des données en **mémoire** (RAM).\n",
    "  \n",
    "- Dask:\n",
    "\n",
    "   - Conçu pour des jeux de données **plus grands que la mémoire** (exécute des calculs parallèles et distribués).\n",
    "  - Utilise la **parallélisation** pour répartir les calculs sur plusieurs cœurs ou machines.\n",
    "  - Permet de manipuler des **ensembles de données massifs** en utilisant des partitions et un calcul différé (lazy evaluation).\n",
    "  - Les opérations sont **similaires à Pandas**, mais Dask permet de travailler avec des jeux de données plus volumineux que la mémoire, et avec des calculs parallèles.\n",
    "\n",
    "\n",
    "\n",
    "### **Exercice Pratique : Traitement Parallèle avec Dask **\n",
    "\n",
    "#### Objectifs :\n",
    "\n",
    "- Charger un fichier CSV volumineux avec Dask.\n",
    "- Effectuer des opérations simples comme l'agrégation, le filtrage ou le calcul de statistiques.\n",
    "- Mesurer la différence de performance entre Pandas et Dask.\n",
    "\n",
    "#### Instructions :\n",
    "\n",
    "1. **Charger un fichier avec Dask**\n",
    "\n",
    "   - Téléchargez un fichier CSV volumineux ou créez-en un (par exemple, 10 à 100 Mo).\n",
    "   - Utilisez `dask.dataframe.read_csv()` pour charger les données en parallèle.\n",
    "\n",
    "   Exemple de chargement de données :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea070739",
   "metadata": {},
   "outputs": [],
   "source": [
    "   import dask.dataframe as dd\n",
    "   \n",
    "   # Charger un fichier CSV avec Dask\n",
    "   df = dd.read_csv('large_data.csv')\n",
    "   \n",
    "   # Visualiser les premières lignes du DataFrame\n",
    "   print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a26882",
   "metadata": {},
   "source": [
    "\n",
    "2. **Effectuer des opérations de filtrage et d'agrégation**\n",
    "\n",
    "   - Appliquez des filtres et des agrégations sur les données. Par exemple :\n",
    "     - Filtrer les lignes où une colonne spécifique est supérieure à une certaine valeur.\n",
    "     - Agréger les données par une colonne et calculer la somme des autres colonnes.\n",
    "\n",
    "   Exemple de filtrage et d'agrégation :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c88791",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Filtrer les lignes où 'column_name' > 50\n",
    "   filtered_df = df[df['column_name'] > 50]\n",
    "   \n",
    "   # Agréger les données par une autre colonne\n",
    "   aggregated_df = filtered_df.groupby('another_column').agg({'value_column': 'sum'})\n",
    "   \n",
    "   # Calculer le résultat\n",
    "   print(aggregated_df.compute())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483c566d",
   "metadata": {},
   "source": [
    "\n",
    "3. **Comparer la performance avec Pandas**\n",
    "\n",
    "   - Essayez de charger et de traiter le même fichier avec **Pandas** pour comparer le temps d'exécution.\n",
    "\n",
    "   Exemple avec Pandas :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbbca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "   import pandas as pd\n",
    "   df_pandas = pd.read_csv('large_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da6b09e",
   "metadata": {},
   "source": [
    "\n",
    "   Comparez le temps d'exécution pour le chargement et les transformations des données entre Dask et Pandas.\n",
    "\n",
    "### Conclusion :\n",
    "\n",
    "- **Dask** est une solution idéale pour gérer des jeux de données volumineux qui ne tiennent pas en mémoire. Il permet de paralléliser les opérations sur les données, rendant ainsi le traitement beaucoup plus rapide.\n",
    "- **Pandas** reste un excellent choix pour des données plus petites ou lorsque la mémoire est suffisante pour contenir l'ensemble de données.\n",
    "- **L'utilisation de Dask** permet d'effectuer des calculs complexes et de traiter de grandes quantités de données de manière efficace, même sur des machines avec des ressources limitées.\n",
    "\n",
    "Cela conclut la première section sur le **traitement parallèle avec Dask**. Nous avons couvert comment utiliser Dask pour traiter de grands volumes de données en parallèle et comment le comparer à Pandas en termes de performance.\n",
    "\n",
    "### Exercice Pratique : Traitement Parallèle avec Dask \n",
    "\n",
    "1. Charger un fichier CSV volumineux avec Dask.\n",
    "2. Appliquer des agrégations et des calculs parallèles sur les données.\n",
    "3. Récupérer les résultats et analyser la performance.\n",
    "\n",
    "#### Instructions :\n",
    "\n",
    "**Étape 1 : Charger un fichier CSV avec Dask**\n",
    "\n",
    "Tout d'abord, vous allez charger un fichier CSV volumineux à l'aide de Dask. Vous pouvez télécharger un fichier CSV ou utiliser un fichier existant. Si vous ne disposez pas d'un gros fichier CSV, vous pouvez en générer un ou télécharger un jeu de données public volumineux (par exemple, un fichier avec des millions de lignes).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a803280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Charger le fichier CSV avec Dask (modifiez le chemin vers le fichier réel)\n",
    "df = dd.read_csv('large_data.csv')\n",
    "\n",
    "# Visualiser les premières lignes pour vérifier le chargement\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a348b6",
   "metadata": {},
   "source": [
    "\n",
    "Dask traitera le fichier par partitions. L'utilisation de `head()` renvoie un échantillon des premières lignes, mais les calculs réels seront effectués uniquement lorsque vous appelerez `.compute()`.\n",
    "\n",
    "**Étape 2 : Appliquer des agrégations et des calculs parallèles**\n",
    "\n",
    "Appliquez des calculs sur les données en utilisant les fonctions d'agrégation et de transformation de Dask. Par exemple, vous pouvez appliquer un **groupby** pour agréger les données par une colonne spécifique et calculer la somme d'une autre colonne.\n",
    "\n",
    "Exemple d'agrégation par groupe :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ac6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supposons que nous avons une colonne 'category' et une colonne 'value'\n",
    "# Nous voulons calculer la somme de 'value' pour chaque catégorie\n",
    "\n",
    "aggregated_df = df.groupby('category')['value'].sum()\n",
    "\n",
    "# Calculer et récupérer les résultats\n",
    "result = aggregated_df.compute()\n",
    "\n",
    "# Afficher les résultats\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f41b72",
   "metadata": {},
   "source": [
    "\n",
    "Ici, nous utilisons `.groupby()` pour regrouper les données par la colonne `'category'` et calculer la somme des valeurs dans la colonne `'value'`. La méthode `.compute()` lance le calcul final et vous donne les résultats.\n",
    "\n",
    "**Étape 3 : Appliquer des calculs parallèles avec Dask**\n",
    "\n",
    "Pour aller plus loin, vous pouvez appliquer des calculs parallèles tels que des **calculs conditionnels** ou d'autres agrégations en parallèle, comme calculer la moyenne ou effectuer des opérations complexes sur les partitions.\n",
    "\n",
    "Voici un exemple de calcul parallèle d'une moyenne :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a348b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la moyenne des valeurs pour chaque catégorie\n",
    "mean_df = df.groupby('category')['value'].mean()\n",
    "\n",
    "# Récupérer les résultats calculés\n",
    "mean_result = mean_df.compute()\n",
    "\n",
    "# Afficher la moyenne\n",
    "print(mean_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de560b0",
   "metadata": {},
   "source": [
    "\n",
    "**Étape 4 : Comparer la performance avec Pandas (optionnel)**\n",
    "\n",
    "Si vous souhaitez comparer la performance de Dask à celle de Pandas, vous pouvez charger le même fichier avec Pandas et mesurer le temps d'exécution. Pour ce faire, vous pouvez utiliser `time` pour mesurer le temps d'exécution des deux versions (Dask et Pandas).\n",
    "\n",
    "Voici un exemple pour charger et traiter le même fichier avec Pandas :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda3479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Mesurer le temps d'exécution pour Pandas\n",
    "start_time = time.time()\n",
    "\n",
    "# Charger le fichier avec Pandas\n",
    "df_pandas = pd.read_csv('large_data.csv')\n",
    "\n",
    "# Appliquer les mêmes calculs d'agrégation avec Pandas\n",
    "result_pandas = df_pandas.groupby('category')['value'].sum()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(result_pandas)\n",
    "\n",
    "# Afficher le temps d'exécution pour Pandas\n",
    "print(f\"Temps d'exécution avec Pandas : {time.time() - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5ce2c",
   "metadata": {},
   "source": [
    "\n",
    "**Conclusion**\n",
    "\n",
    "- Dask permet de traiter de grands volumes de données en **parallèle**, rendant les opérations beaucoup plus rapides que si elles étaient effectuées sur un seul thread.\n",
    "- Avec Dask, les opérations sont **déléguées à des partitions** et les calculs ne sont effectués qu'avec `.compute()`, ce qui permet une **exécution paresseuse**.\n",
    "- Comparer les performances entre **Pandas** et **Dask** vous montre comment Dask peut être plus efficace lorsqu'on travaille avec des ensembles de données qui ne tiennent pas en mémoire.\n",
    "\n",
    "**Bonus** : Si vous travaillez sur un **cluster** de machines, vous pouvez utiliser Dask pour exécuter des calculs distribués sur plusieurs nœuds.\n",
    "\n",
    "### Partie 5.2 : Optimisation du Traitement de Données Très Volumineuses \n",
    "\n",
    "Cette section aborde des techniques avancées pour traiter des volumes de données massifs de manière efficace. Vous apprendrez à utiliser des formats de stockage optimisés, des bibliothèques accélérées, ainsi que des solutions de stockage dans le cloud pour améliorer la gestion et le traitement des grandes quantités de données.\n",
    "\n",
    "**1. Utilisation de Formats de Stockage Optimisés**\n",
    "\n",
    "Lors du traitement de données très volumineuses, l'utilisation de formats de stockage efficaces devient essentielle pour réduire l'espace disque, améliorer les temps de chargement et faciliter les calculs. Nous allons explorer trois formats optimisés populaires : **HDF5**, **Parquet** et **Feather**.\n",
    "\n",
    "##### **1.1 HDF5 (Hierarchical Data Format version 5)**\n",
    "\n",
    "- **Caractéristiques** :\n",
    "  - Utilisé pour stocker de grands ensembles de données, principalement dans des formats hiérarchiques.\n",
    "  - Idéal pour les données numériques et les matrices de données massives.\n",
    "  - Prise en charge de la compression pour économiser de l'espace disque.\n",
    "  - Lecture et écriture rapides, notamment avec des bibliothèques comme `h5py` et `pandas`.\n",
    "- **Exemple d'utilisation avec Pandas** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b9b6a6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sauvegarder un DataFrame au format HDF5\n",
    "df.to_hdf('data/data.h5', key='df', mode='w')\n",
    "\n",
    "# Charger un DataFrame depuis un fichier HDF5\n",
    "df_loaded = pd.read_hdf('data/data.h5', key='df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ed52f303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Nom</th>\n",
       "      <th>prenom</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said 1</td>\n",
       "      <td>xx 1</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>said 2</td>\n",
       "      <td>xx 2</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>said 3</td>\n",
       "      <td>xx 3</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>said 4</td>\n",
       "      <td>xx 4</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>said 5</td>\n",
       "      <td>xx 5</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Nom  prenom  age\n",
       "0  said 1   xx 1   59\n",
       "1  said 2   xx 2   32\n",
       "2  said 3   xx 3   50\n",
       "3  said 4   xx 4   28\n",
       "4  said 5   xx 5   74"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_loaded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5f4b2",
   "metadata": {},
   "source": [
    "\n",
    "##### **1.2 Parquet**\n",
    "\n",
    "- **Caractéristiques** :\n",
    "  - Format de stockage de données colonnes, idéal pour les données tabulaires.\n",
    "  - Très efficace en termes de stockage et de compression.\n",
    "  - Prise en charge des métadonnées et de la structuration des données.\n",
    "  - Utilisé par des systèmes distribués comme Apache Spark.\n",
    "- **Exemple d'utilisation avec Pandas** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "eed28f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sauvegarder un DataFrame au format Parquet\n",
    "df.to_parquet('data/data.parquet')\n",
    "\n",
    "# Charger un DataFrame depuis un fichier Parquet\n",
    "df_loaded = pd.read_parquet('data/data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bb726",
   "metadata": {},
   "source": [
    "\n",
    "##### **1.3 Feather**\n",
    "\n",
    "- **Caractéristiques** :\n",
    "  - Format de fichier rapide à lire et écrire, conçu pour les petites et moyennes tailles de données.\n",
    "  - Peut être particulièrement utile pour l'échange de données entre différentes sessions et langages de programmation (Pandas et R).\n",
    "- **Exemple d'utilisation avec Pandas** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "74f50643",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_feather'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Sauvegarder un DataFrame au format Feather\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_feather\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/data.feather\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Charger un DataFrame depuis un fichier Feather\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df_loaded \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_feather(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/data.feather\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\dask\\dataframe\\core.py:4915\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4913\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m   4914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4915\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m key)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_feather'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sauvegarder un DataFrame au format Feather\n",
    "df.to_feather('data/data.feather')\n",
    "\n",
    "# Charger un DataFrame depuis un fichier Feather\n",
    "df_loaded = pd.read_feather('data/data.feather')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4492265",
   "metadata": {},
   "source": [
    "\n",
    "##### **Pourquoi utiliser ces formats ?**\n",
    "\n",
    "- **Performance** : Les formats comme Parquet et HDF5 sont optimisés pour le stockage des données et les accès rapides, permettant un traitement plus rapide par rapport à des formats comme CSV.\n",
    "- **Compression** : Ils offrent une meilleure compression sans perte de données, réduisant ainsi l'utilisation de l'espace disque.\n",
    "- **Gestion des données volumineuses** : Ces formats sont conçus pour traiter de grandes quantités de données efficacement, avec des temps de lecture et d'écriture significativement plus rapides que les formats classiques.\n",
    "\n",
    "#### **2. Accélération du Traitement avec Cython et Modin**\n",
    "\n",
    "##### **2.1 Cython : Accélération des Opérations Python**\n",
    "\n",
    "Cython est un langage qui permet de convertir du code Python en C pour obtenir des gains de performance significatifs, notamment pour les boucles et les fonctions qui nécessitent des calculs intensifs.\n",
    "\n",
    "- **Exemple d'utilisation de Cython** :\n",
    "\n",
    "1. Créez un fichier `example.pyx` avec le code Python à accélérer.\n",
    "2. Compilez ce fichier avec Cython pour obtenir une version compilée en C.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a213e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example.pyx\n",
    "def somme(a, b):\n",
    "    return a + b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aa2356",
   "metadata": {},
   "source": [
    "\n",
    "Ensuite, compilez le fichier en utilisant Cython et l'outil `setup.py`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "add69237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling bibliothEx/maths.pyx because it changed.\n",
      "[1/1] Cythonizing bibliothEx/maths.pyx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\Cython\\Compiler\\Main.py:381: FutureWarning: Cython directive 'language_level' not set, using '3str' for now (Py3). This has changed from earlier releases! File: d:\\OneDrive - Université Sultan Moulay Slimane\\cours excellence\\python\\bibliothEx\\maths.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\setuptools\\_distutils\\fancy_getopt.py:245\u001b[0m, in \u001b[0;36mFancyGetopt.getopt\u001b[1;34m(self, args, object)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 245\u001b[0m     opts, args \u001b[38;5;241m=\u001b[39m \u001b[43mgetopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetopt\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshort_opts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong_opts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m getopt\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m msg:\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\getopt.py:93\u001b[0m, in \u001b[0;36mgetopt\u001b[1;34m(args, shortopts, longopts)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 93\u001b[0m     opts, args \u001b[38;5;241m=\u001b[39m \u001b[43mdo_longs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlongopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\getopt.py:164\u001b[0m, in \u001b[0;36mdo_longs\u001b[1;34m(opts, opt, longopts, args)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m optarg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GetoptError(_(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moption --\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must not have an argument\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m%\u001b[39m opt, opt)\n\u001b[0;32m    165\u001b[0m opts\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m opt, optarg \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mGetoptError\u001b[0m: option --fullname must not have an argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDistutilsArgError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\setuptools\\_distutils\\core.py:170\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     ok \u001b[38;5;241m=\u001b[39m \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_command_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DistutilsArgError \u001b[38;5;28;01mas\u001b[39;00m msg:\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\setuptools\\_distutils\\dist.py:464\u001b[0m, in \u001b[0;36mDistribution.parse_command_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m parser\u001b[38;5;241m.\u001b[39mset_aliases({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicence\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlicense\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m--> 464\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetopt\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscript_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    465\u001b[0m option_order \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mget_option_order()\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\setuptools\\_distutils\\fancy_getopt.py:247\u001b[0m, in \u001b[0;36mFancyGetopt.getopt\u001b[1;34m(self, args, object)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m getopt\u001b[38;5;241m.\u001b[39merror \u001b[38;5;28;01mas\u001b[39;00m msg:\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DistutilsArgError(msg)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m opt, val \u001b[38;5;129;01min\u001b[39;00m opts:\n",
      "\u001b[1;31mDistutilsArgError\u001b[0m: option --fullname must not have an argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[150], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msetuptools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup\n\u001b[1;32m----> 4\u001b[0m \u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mext_modules\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcythonize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbibliothEx/maths.pyx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\setuptools\\__init__.py:117\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    116\u001b[0m _install_setup_requires(attrs)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\setuptools\\_distutils\\core.py:172\u001b[0m, in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DistutilsArgError \u001b[38;5;28;01mas\u001b[39;00m msg:\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(gen_usage(dist\u001b[38;5;241m.\u001b[39mscript_name) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124merror: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n",
      "\u001b[1;31mSystemExit\u001b[0m: usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n   or: ipykernel_launcher.py --help-commands\n   or: ipykernel_launcher.py cmd --help\n\nerror: option --fullname must not have an argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2095\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2093\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2094\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2095\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2096\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2098\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2099\u001b[0m         \u001b[38;5;66;03m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   2100\u001b[0m         \u001b[38;5;66;03m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   2101\u001b[0m         \u001b[38;5;66;03m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\ultratb.py:696\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    689\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \n\u001b[0;32m    691\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\ultratb.py:559\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    556\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    557\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    558\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 559\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    563\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    566\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\ultratb.py:1396\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1395\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m etb\n\u001b[1;32m-> 1396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1397\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1398\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\ultratb.py:1287\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1284\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1286\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\ultratb.py:1140\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1133\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1137\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1138\u001b[0m ):\n\u001b[0;32m   1139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1140\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\ultratb.py:1030\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m   1028\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(\u001b[38;5;28mstr\u001b[39m(etype), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m   1029\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1030\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1033\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1034\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\IPython\\core\\ultratb.py:1098\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1098\u001b[0m         mod \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43mcf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1099\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m             mod_name \u001b[38;5;241m=\u001b[39m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "from Cython.Build import cythonize\n",
    "from setuptools import setup\n",
    "\n",
    "setup(\n",
    "    ext_modules = cythonize(\"bibliothEx/maths.pyx\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7985885",
   "metadata": {},
   "source": [
    "\n",
    "Ensuite, vous pouvez utiliser la fonction compilée dans votre code principal.\n",
    "\n",
    "##### **2.2 Modin : Accélération de Pandas en Parallèle**\n",
    "\n",
    "Modin est une bibliothèque qui accélère l'exécution des opérations Pandas en utilisant le parallélisme sur plusieurs cœurs de processeur. Il peut être utilisé avec minimalement de modifications au code existant Pandas.\n",
    "\n",
    "- **Installation de Modin** :\n",
    "\n",
    "```bash\n",
    "pip install modin[ray]\n",
    "```\n",
    "\n",
    "- **Exemple d'utilisation** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "5c23d163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open file: d:/OneDrive - Université Sultan Moulay Slimane/cours excellence/python/data/volumes.csv\n",
      "Partition ID: e03447cdec5044c49369ae19a188f13e, Height: None, Width: 3, Node IP: None\n",
      "Partition ID: 86d76b8ceb2b47c29b1593db64ef3df2, Height: None, Width: 3, Node IP: None\n",
      "Partition ID: 3782a3e9b3044b5d88afeb71ee4427ae, Height: None, Width: 3, Node IP: None\n",
      "Partition ID: 343ea8c768944cacbbdb79e3266799e0, Height: None, Width: 3, Node IP: None\n",
      "Partition ID: f158637d03ad47349f9f16c32a01ee59, Height: None, Width: 3, Node IP: None\n",
      "Partition ID: 5a07bd36f4d34c22b8a67bc03c505e05, Height: None, Width: 3, Node IP: None\n",
      "Partition ID: 1c2bd9d8ebb8409184ecea2f78f645c7, Height: None, Width: 3, Node IP: None\n",
      "Partition ID: 157e0160218c4fd99d0ba929b0d347a4, Height: None, Width: 3, Node IP: None\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'category'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/volumes.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Effectuer des opérations parallélisées (comme groupby, agrégations)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcategory\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\logging\\logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[0;32m    131\u001b[0m logger_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\pandas\\dataframe.py:472\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m    470\u001b[0m         level, by \u001b[38;5;241m=\u001b[39m by, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 472\u001b[0m         by \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_query_compiler\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(by, Series):\n\u001b[0;32m    474\u001b[0m     drop \u001b[38;5;241m=\u001b[39m by\u001b[38;5;241m.\u001b[39m_parent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\logging\\logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[0;32m    131\u001b[0m logger_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\pandas\\base.py:3555\u001b[0m, in \u001b[0;36mBasePandasDataset.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(indexer)\n\u001b[0;32m   3554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\logging\\logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[0;32m    131\u001b[0m logger_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\pandas\\dataframe.py:2979\u001b[0m, in \u001b[0;36mDataFrame._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_to_pandas(pandas\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m, key)\n\u001b[0;32m   2977\u001b[0m     \u001b[38;5;66;03m# return self._getitem_multilevel(key)\u001b[39;00m\n\u001b[0;32m   2978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\logging\\logger_decorator.py:128\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[0;32m    131\u001b[0m logger_level \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(logger, log_level)\n",
      "File \u001b[1;32mc:\\Users\\fpbm1\\.conda\\envs\\itga2024\\lib\\site-packages\\modin\\pandas\\dataframe.py:2362\u001b[0m, in \u001b[0;36mDataFrame._getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2348\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2349\u001b[0m \u001b[38;5;124;03mGet column specified by `key`.\u001b[39;00m\n\u001b[0;32m   2350\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[38;5;124;03m    Selected column.\u001b[39;00m\n\u001b[0;32m   2360\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 2362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(key))\n\u001b[0;32m   2363\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__constructor__(\n\u001b[0;32m   2364\u001b[0m     query_compiler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_compiler\u001b[38;5;241m.\u001b[39mgetitem_column_array([key])\n\u001b[0;32m   2365\u001b[0m )\u001b[38;5;241m.\u001b[39msqueeze(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2366\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, Series):\n",
      "\u001b[1;31mKeyError\u001b[0m: 'category'"
     ]
    }
   ],
   "source": [
    "import modin.pandas as mpd\n",
    "\n",
    "# Charger un DataFrame avec Modin (fonctionne comme Pandas)\n",
    "df = mpd.read_csv('data/volumes.csv')\n",
    "\n",
    "# Effectuer des opérations parallélisées (comme groupby, agrégations)\n",
    "result = df.groupby('category')['value'].sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fae124c",
   "metadata": {},
   "source": [
    "\n",
    "Modin exécute les calculs en parallèle sur tous les cœurs du processeur, ce qui améliore considérablement les performances des opérations sur de grandes quantités de données.\n",
    "\n",
    "#### **3. Stockage et Gestion des Données dans le Cloud**\n",
    "\n",
    "Le stockage des données dans le cloud est essentiel pour travailler avec des jeux de données volumineux qui ne peuvent pas être stockés localement. Les services comme **Amazon S3**, **Azure Blob Storage** et **Google Cloud Storage** permettent de stocker, gérer et traiter de grandes quantités de données à une échelle sans avoir à gérer des serveurs physiques.\n",
    "\n",
    "##### **3.1 Exemple de stockage avec Amazon S3**\n",
    "\n",
    "Amazon S3 est une solution de stockage en nuage permettant de stocker de grands volumes de données et d'y accéder de manière efficace. Avec `boto3`, une bibliothèque Python, vous pouvez interagir avec S3 pour charger et enregistrer des données.\n",
    "\n",
    "- **Installation de boto3** :\n",
    "\n",
    "```bash\n",
    "pip install boto3\n",
    "```\n",
    "\n",
    "- **Exemple d’utilisation de S3 avec Pandas** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3275e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Créer une session S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Télécharger un fichier CSV depuis S3\n",
    "s3.download_file('my-bucket', 'data.csv', 'data.csv')\n",
    "\n",
    "# Charger le fichier CSV dans un DataFrame Pandas\n",
    "df = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a36435",
   "metadata": {},
   "source": [
    "\n",
    "##### **3.2 Exemple avec Azure Blob Storage**\n",
    "\n",
    "Azure Blob Storage est un autre service cloud utilisé pour stocker des données non structurées. Il fonctionne de manière similaire à S3, permettant le stockage de données volumineuses et leur gestion efficace.\n",
    "\n",
    "- **Installation de Azure Storage Blob** :\n",
    "\n",
    "```bash\n",
    "pip install azure-storage-blob\n",
    "```\n",
    "\n",
    "- **Exemple d’utilisation avec Azure Blob** :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33966f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Connexion à Azure Blob Storage\n",
    "blob_service_client = BlobServiceClient.from_connection_string('your_connection_string')\n",
    "blob_client = blob_service_client.get_blob_client(container='your_container', blob='data.csv')\n",
    "\n",
    "# Télécharger le fichier\n",
    "with open('data.csv', 'wb') as f:\n",
    "    f.write(blob_client.download_blob().readall())\n",
    "\n",
    "# Charger le fichier dans un DataFrame\n",
    "df = pd.read_csv('data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1c310",
   "metadata": {},
   "source": [
    "\n",
    "##### **Avantages du Cloud pour le Traitement des Données** :\n",
    "\n",
    "- **Scalabilité** : Vous pouvez stocker et traiter d'énormes quantités de données sans limitation d’espace physique.\n",
    "- **Accessibilité** : L’accès aux données est facile depuis n'importe quel endroit, avec des outils d'optimisation intégrés pour le traitement.\n",
    "- **Sécurité** : Les services cloud offrent des fonctionnalités de sécurité pour garantir la protection des données.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Le traitement de données volumineuses nécessite l’utilisation de formats de stockage optimisés, de bibliothèques accélérées comme Cython et Modin, ainsi que d'infrastructures cloud pour stocker et gérer les données de manière efficace. L’adoption de ces technologies vous permettra de traiter des ensembles de données massifs avec des gains de performance significatifs, tout en garantissant la scalabilité et l’accessibilité des données.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itga2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
